{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQDOPjL61Y0K"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This script demonstrates how you can use a RNN-based model to detect anomalies in timeseries data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZQ1Zy1h1Y0L"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:53:43.472225Z",
     "iopub.status.busy": "2023-03-23T13:53:43.471296Z",
     "iopub.status.idle": "2023-03-23T13:53:45.190895Z",
     "shell.execute_reply": "2023-03-23T13:53:45.190070Z",
     "shell.execute_reply.started": "2023-03-23T13:53:43.472168Z"
    },
    "id": "0FfTR9ku1Y0L",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 22:53:44.122759: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from evaluator import evaluate, compute_threshold\n",
    "from dataloader import loader\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9sE3S3G1Y0M"
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:53:45.192127Z",
     "iopub.status.busy": "2023-03-23T13:53:45.191720Z",
     "iopub.status.idle": "2023-03-23T13:53:45.273116Z",
     "shell.execute_reply": "2023-03-23T13:53:45.272243Z",
     "shell.execute_reply.started": "2023-03-23T13:53:45.192109Z"
    },
    "id": "EtSnZuX_1Y0M",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train, df_test, df_test_label = loader(dataset=\"SMAP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PZGCGNM1Y0O"
   },
   "source": [
    "## Prepare training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:53:45.274925Z",
     "iopub.status.busy": "2023-03-23T13:53:45.274616Z",
     "iopub.status.idle": "2023-03-23T13:53:45.390848Z",
     "shell.execute_reply": "2023-03-23T13:53:45.390039Z",
     "shell.execute_reply.started": "2023-03-23T13:53:45.274908Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 135183\n",
      "Number of training samples: 427617\n"
     ]
    }
   ],
   "source": [
    "# Skip data normalization because SMAP datas were already normalized.\n",
    "df_train = df_train.fillna(method=\"ffill\")\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train)\n",
    "df_training_value = pd.DataFrame(scaler.transform(df_train))\n",
    "df_test_value = pd.DataFrame(scaler.transform(df_test))\n",
    "print(\"Number of training samples:\", len(df_training_value))\n",
    "print(\"Number of training samples:\", len(df_test_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76o15gMK1Y0P"
   },
   "source": [
    "### Create sequences\n",
    "Create sequences combining `TIME_STEPS` contiguous data values from the\n",
    "training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:53:45.392145Z",
     "iopub.status.busy": "2023-03-23T13:53:45.391817Z",
     "iopub.status.idle": "2023-03-23T13:53:47.616036Z",
     "shell.execute_reply": "2023-03-23T13:53:47.615255Z",
     "shell.execute_reply.started": "2023-03-23T13:53:45.392128Z"
    },
    "id": "Z_wb3M4Q1Y0P",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input shape:  (135084, 100, 25)\n",
      "Test input shape:  (427518, 100, 25)\n"
     ]
    }
   ],
   "source": [
    "TIME_STEPS = 100\n",
    "\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps=TIME_STEPS):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "\n",
    "x_train = create_sequences(df_training_value.values)\n",
    "print(\"Training input shape: \", x_train.shape)\n",
    "\n",
    "# Create sequences from test values.\n",
    "x_test = create_sequences(df_test_value.values)\n",
    "print(\"Test input shape: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnBIFj7w1Y0P"
   },
   "source": [
    "## Build a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:53:47.617220Z",
     "iopub.status.busy": "2023-03-23T13:53:47.616913Z",
     "iopub.status.idle": "2023-03-23T13:53:47.623317Z",
     "shell.execute_reply": "2023-03-23T13:53:47.622612Z",
     "shell.execute_reply.started": "2023-03-23T13:53:47.617203Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(keras.Model):\n",
    "    def __init__(self, time_step, x_dim, h1_dim, h2_dim, name=\"generator\", **kwargs):\n",
    "        super(Generator, self).__init__(name=name, **kwargs)\n",
    "        self.generator_inputs = keras.Input(shape=(time_step, x_dim))\n",
    "        self.generator_gru1 = layers.GRU(h1_dim, dropout=0.2, return_sequences=\"True\")\n",
    "        self.generator_gru2 = layers.GRU(h2_dim)\n",
    "        self.generator_repeat = layers.RepeatVector(time_step)\n",
    "        self.generator_gru3 = layers.GRU(h2_dim, dropout=0.2, return_sequences=\"True\")\n",
    "        self.generator_gru4 = layers.GRU(h1_dim)\n",
    "        self.generator_dense = layers.Dense(time_step * x_dim)\n",
    "        self.reshape = layers.Reshape([time_step, x_dim])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.generator_inputs = inputs\n",
    "        hidden1 = self.generator_gru1(self.generator_inputs)\n",
    "        hidden2 = self.generator_gru2(hidden1)\n",
    "        repeat = self.generator_repeat(hidden2)\n",
    "        hidden3 = self.generator_gru3(repeat)\n",
    "        hidden4 = self.generator_gru4(hidden3)\n",
    "        flatten = self.generator_dense(hidden4)\n",
    "        generated_x = self.reshape(flatten)\n",
    "\n",
    "        return generated_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:53:47.624401Z",
     "iopub.status.busy": "2023-03-23T13:53:47.624125Z",
     "iopub.status.idle": "2023-03-23T13:53:47.628963Z",
     "shell.execute_reply": "2023-03-23T13:53:47.628336Z",
     "shell.execute_reply.started": "2023-03-23T13:53:47.624386Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(keras.Model):\n",
    "    def __init__(self, time_step, x_dim, h_dim, name=\"discriminator\", **kwargs):\n",
    "        super(Discriminator, self).__init__(name=name, **kwargs)\n",
    "        self.discriminator_inputs = keras.Input(shape=(time_step, x_dim))\n",
    "        self.discriminator_gru1 = layers.GRU(h_dim, dropout=0.2, return_sequences=\"True\")\n",
    "        self.discriminator_gru2 = layers.GRU(h_dim, dropout=0.2)\n",
    "        self.discriminator_dense = layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.discriminator_inputs = inputs\n",
    "        hidden1 = self.discriminator_gru1(self.discriminator_inputs)\n",
    "        hidden2 = self.discriminator_gru2(hidden1)\n",
    "        result = self.discriminator_dense(hidden2)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:53:47.629892Z",
     "iopub.status.busy": "2023-03-23T13:53:47.629628Z",
     "iopub.status.idle": "2023-03-23T13:53:48.706831Z",
     "shell.execute_reply": "2023-03-23T13:53:48.706054Z",
     "shell.execute_reply.started": "2023-03-23T13:53:47.629877Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 22:53:47.635776: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-03-23 22:53:47.693368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:1f:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6\n",
      "coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s\n",
      "2023-03-23 22:53:47.693636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:20:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6\n",
      "coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s\n",
      "2023-03-23 22:53:47.693663: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-03-23 22:53:47.696290: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-03-23 22:53:47.696368: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-03-23 22:53:47.697174: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2023-03-23 22:53:47.697424: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2023-03-23 22:53:47.698044: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-03-23 22:53:47.698591: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-03-23 22:53:47.698735: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-03-23 22:53:47.699630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2023-03-23 22:53:47.699970: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-23 22:53:47.897146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:1f:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6\n",
      "coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s\n",
      "2023-03-23 22:53:47.897704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:20:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6\n",
      "coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s\n",
      "2023-03-23 22:53:47.898480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2023-03-23 22:53:47.898526: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-03-23 22:53:48.655768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-03-23 22:53:48.655803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 \n",
      "2023-03-23 22:53:48.655808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N N \n",
      "2023-03-23 22:53:48.655811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   N N \n",
      "2023-03-23 22:53:48.657108: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-03-23 22:53:48.657137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1533 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:1f:00.0, compute capability: 8.6)\n",
      "2023-03-23 22:53:48.657770: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-03-23 22:53:48.657790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 22312 MB memory) -> physical GPU (device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:20:00.0, compute capability: 8.6)\n"
     ]
    }
   ],
   "source": [
    "generator = Generator(TIME_STEPS, x_train.shape[2], 128, 64, dtype=\"float32\")\n",
    "discriminator = Discriminator(TIME_STEPS, x_train.shape[2], 64, dtype=\"float32\")\n",
    "\n",
    "gan = keras.models.Sequential([generator, discriminator])\n",
    "\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "discriminator.trainable = False\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Unm8M4T71Y0P",
    "tags": []
   },
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:53:48.707896Z",
     "iopub.status.busy": "2023-03-23T13:53:48.707622Z",
     "iopub.status.idle": "2023-03-23T13:53:48.713148Z",
     "shell.execute_reply": "2023-03-23T13:53:48.712472Z",
     "shell.execute_reply.started": "2023-03-23T13:53:48.707874Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for training GAN\n",
    "# Ref: https://github.com/young-eun-nam/Mr.TAD/blob/main/Baseline%20-%20LSTM-GAN.ipynb\n",
    "def train_gan(gan, dataset, batch_size, codings_size, dim, n_epochs=50):\n",
    "    generator, discriminator = gan.layers\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        for X_batch in dataset:\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size, dim])\n",
    "            generated_images = generator(noise)\n",
    "            X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)\n",
    "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
    "            discriminator.trainable = True\n",
    "            discriminator.train_on_batch(X_fake_and_real, y1)\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size, dim])\n",
    "            y2 = tf.constant([[1.]] * batch_size)\n",
    "            discriminator.trainable = False\n",
    "            gan.train_on_batch(noise, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T13:53:48.714129Z",
     "iopub.status.busy": "2023-03-23T13:53:48.713862Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e41a9e96234b6888d62466fc4acfc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 22:53:51.152702: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-03-23 22:53:51.793596: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8201\n",
      "2023-03-23 22:53:51.958187: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-03-23 22:53:52.571020: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-03-23 22:53:52.571080: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-03-23 22:53:53.640166: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-03-23 22:53:53.668581: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2900000000 Hz\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "dataset = dataset.batch(128, drop_remainder=True).prefetch(1)\n",
    "\n",
    "train_gan(gan, dataset, 128, x_train.shape[1], x_train.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUyjNuQk1Y0Q",
    "tags": []
   },
   "source": [
    "## Detecting anomalies\n",
    "### We could set threshold my using train loss, and detect which points are anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgK1BMu_1Y0Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get prediction of train data\n",
    "x_train_pred = gan.layers[0].predict(x_train)\n",
    "print(\"Predict train data done\")\n",
    "\n",
    "# Get test MAE loss.\n",
    "x_test_pred = gan.layers[0].predict(x_test)\n",
    "print(\"Predict test data done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    \"abs_mean\": compute_threshold(x_train, x_train_pred, option=\"abs_mean\"),\n",
    "    \"abs_median\": compute_threshold(x_train, x_train_pred, option=\"abs_median\"),\n",
    "    \"abs_max\": compute_threshold(x_train, x_train_pred, option=\"abs_max\"),\n",
    "    \"square_mean\": compute_threshold(x_train, x_train_pred, option=\"square_mean\"),\n",
    "    \"square_median\": compute_threshold(x_train, x_train_pred, option=\"square_median\"),\n",
    "    \"rank\": compute_threshold(x_train, x_train_pred, option=\"rank\", label=df_test_label[\"label\"]),\n",
    "}\n",
    "print(f'abs mean threshold: {thresholds[\"abs_mean\"]:.3f}')\n",
    "print(f'abs median threshold: {thresholds[\"abs_median\"]:.3f}')\n",
    "print(f'abs max threshold: {thresholds[\"abs_max\"]:.3f}')\n",
    "print(f'square mean threshold: {thresholds[\"square_mean\"]:.3f}')\n",
    "print(f'square median threshold: {thresholds[\"square_median\"]:.3f}')\n",
    "print(f'rank threshold: {thresholds[\"rank\"]:.3f}')\n",
    "\n",
    "# Detect all the samples which are anomalies.\n",
    "test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose the lowest loss of the anomaly datas((# of anomalies)th) as threshold by using rank option\n",
    "threshold = thresholds[\"rank\"]\n",
    "print(f\"threshold: {threshold:.3f}\")\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "labels = df_test_label[\"label\"].values.tolist()\n",
    "dates = df_test.index\n",
    "\n",
    "\n",
    "pd.DataFrame(np.sum(test_mae_loss, axis=1)).plot(ax=ax)\n",
    "pd.DataFrame([threshold] * len(df_test)).plot(ax=ax)\n",
    "ax.legend([\"reconstruction error\", \"threshold\"], loc=\"upper right\")\n",
    "\n",
    "\n",
    "temp_start = dates[0]\n",
    "temp_date = dates[0]\n",
    "temp_label = labels[0]\n",
    "\n",
    "for xc, value in zip(dates, labels):\n",
    "    if temp_label != value:\n",
    "        if temp_label == True:\n",
    "            ax.axvspan(temp_start, temp_date, alpha=0.2, color=\"orange\")\n",
    "        temp_start = xc\n",
    "        temp_label = value\n",
    "    temp_date = xc\n",
    "if temp_label == True:\n",
    "    ax.axvspan(temp_start, xc, alpha=0.2, color=\"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kW_E_0AZ1Y0R",
    "tags": []
   },
   "source": [
    "## Predict & Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:38:01.255924Z",
     "iopub.status.busy": "2023-02-11T12:38:01.255364Z",
     "iopub.status.idle": "2023-02-11T12:38:01.261595Z",
     "shell.execute_reply": "2023-02-11T12:38:01.260128Z",
     "shell.execute_reply.started": "2023-02-11T12:38:01.255879Z"
    },
    "tags": []
   },
   "source": [
    "## #1 if any included time step has anomalies => abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKbjg85P1Y0R",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies\n",
    "anomalies = np.sum(test_mae_loss, axis=1) > threshold\n",
    "print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
    "\n",
    "anomalous_data_indices = []\n",
    "for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):\n",
    "    if np.all(anomalies[data_idx - TIME_STEPS + 1 : data_idx]):\n",
    "        anomalous_data_indices.append(data_idx)\n",
    "print(\"Number of anomalous samples: \", len(anomalous_data_indices))\n",
    "\n",
    "test_pred = np.zeros(len(df_test))\n",
    "test_label = df_test_label[\"label\"]\n",
    "test_pred[anomalous_data_indices] = 1\n",
    "\n",
    "accuracy = (np.sum(test_pred == test_label)) / len(test_label)\n",
    "precision = (np.sum(test_pred * test_label)) / np.sum(test_pred)\n",
    "recall = (np.sum(test_pred * test_label)) / np.sum(test_label)\n",
    "f1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "\n",
    "print(f\"accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"precision: {precision*100:.2f}%\")\n",
    "print(f\"recall: {recall*100:.2f}%\")\n",
    "print(f\"f1: {f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:38:01.255924Z",
     "iopub.status.busy": "2023-02-11T12:38:01.255364Z",
     "iopub.status.idle": "2023-02-11T12:38:01.261595Z",
     "shell.execute_reply": "2023-02-11T12:38:01.260128Z",
     "shell.execute_reply.started": "2023-02-11T12:38:01.255879Z"
    },
    "tags": []
   },
   "source": [
    "## #2 If half of included time steps has anomaly => abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data i is an anomaly if ratio of anomalies sequences/involved total sequences >= 0.5\n",
    "anomalies = np.sum(test_mae_loss, axis=1) > threshold\n",
    "test_anomalies = np.zeros(len(df_test))\n",
    "test_seqs = np.zeros(len(df_test))\n",
    "test_label = df_test_label[\"label\"]\n",
    "print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
    "\n",
    "for idx, anomaly in enumerate(anomalies):\n",
    "    test_seqs[idx : min(idx + TIME_STEPS, len(df_test))] += 1\n",
    "    if anomaly:\n",
    "        test_anomalies[idx : min(idx + TIME_STEPS, len(df_test))] += 1\n",
    "test_pred_ratio = np.divide(test_anomalies, test_seqs)\n",
    "test_pred = test_pred_ratio >= 0.5\n",
    "print(\"Number of anomalous samples: \", sum(test_pred))\n",
    "\n",
    "\n",
    "accuracy = (np.sum(test_pred == test_label)) / len(test_label)\n",
    "precision = (np.sum(test_pred * test_label)) / np.sum(test_pred)\n",
    "recall = (np.sum(test_pred * test_label)) / np.sum(test_label)\n",
    "f1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "\n",
    "print(f\"accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"precision: {precision*100:.2f}%\")\n",
    "print(f\"recall: {recall*100:.2f}%\")\n",
    "print(f\"f1: {f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 4690\n",
    "evaluation_array = [(test_seqs - test_anomalies)[idx:idx+400],\n",
    "                    test_anomalies[idx:idx+400],\n",
    "                    test_seqs[idx:idx+400],\n",
    "                    test_pred_ratio[idx:idx+400],\n",
    "                    test_pred[idx:idx+400],\n",
    "                    df_test_label[\"label\"][idx:idx+400]]\n",
    "\n",
    "df = pd.DataFrame(evaluation_array, dtype=float)\n",
    "df.index = ['Normal', 'Anomaly', '#Seq', 'Pred(%)', 'Pred', 'GT']\n",
    "df.style.background_gradient(cmap='summer', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kW_E_0AZ1Y0R"
   },
   "source": [
    "### #3 we could find the threshold with highest f1 score by calculating all cases \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from evaluator import evaluate\n",
    "\n",
    "label = create_sequences(df_test_label[\"label\"], TIME_STEPS)\n",
    "print(label.shape)\n",
    "scores = evaluate(x_test, x_test_pred, label, n=10, scoring=\"abs_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"f1:\",np.max(scores['f1']))\n",
    "idx = np.argmax(scores[\"f1\"])\n",
    "\n",
    "print(f\"threshold: {scores['thresholds'][idx]}\")\n",
    "print(f\"precision: {scores['precision'][idx]*100:.2f}%\")\n",
    "print(f\"recall: {scores['recall'][idx]*100:.2f}%\")\n",
    "print(f\"f1: {scores['f1'][idx]*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "timeseries_anomaly_detection",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "cs495_winter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
