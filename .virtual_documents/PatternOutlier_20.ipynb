import pandas as pd
import numpy as np
from tqdm.notebook import tqdm
import os
import seaborn as sns
from matplotlib import pyplot as plt
sns.set(style='darkgrid')


data_path = './length20'
datasets = sorted([f for f in os.listdir(f'{data_path}/test') if os.path.isfile(os.path.join(f'{data_path}/test', f))])


for i, data in tqdm(enumerate(datasets)):
    df = pd.read_csv(f'{data_path}/test/{data}')
    plt.figure(figsize=(20, 5))
    plt.plot(df.iloc[:, i])
    plt.title(f"{data}")
    label_index = np.where(df.iloc[:,-1]==1)
    plt.vlines(label_index, np.min(df.iloc[:, i])-1.0, np.min(df.iloc[:, i])-0.1,color = 'red', linestyles='solid', label='Ground Truth')
    plt.legend(loc='upper center', ncol=3, bbox_to_anchor=(0.91, 1.1))
    plt.plot()


import os
import random

os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID";
os.environ["CUDA_VISIBLE_DEVICES"] = "3"
os.environ["TF_FORCE_GPU_ALLOW_GROWTH"] = "true"


from tensorflow.python.client import device_lib
device_lib.list_local_devices()


import numpy as np
import pandas as pd
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tqdm.notebook import tqdm

# THESE LINES ARE FOR REPRODUCIBILITY
# random.seed(0)
# np.random.seed(0)
# tf.random.set_seed(0)


def GRU_AE(X_train):
    GRU = layers.GRU
    model = keras.Sequential(
        [
            layers.InputLayer(input_shape=(X_train.shape[1], X_train.shape[2])),
            GRU(64, return_sequences=True),
            GRU(32),
            layers.RepeatVector(X_train.shape[1]),
            GRU(32, return_sequences=True),
            GRU(64),
            layers.Dense(X_train.shape[1] *  X_train.shape[2]),
            layers.Reshape([X_train.shape[1], X_train.shape[2]])
        ]
    )
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss="mse")
    history = model.fit(X_train, X_train, epochs=50, batch_size=128, validation_split=0.3, verbose=0, callbacks=[keras.callbacks.EarlyStopping(monitor="val_loss", patience=5, mode="min", restore_best_weights=True)])
    return model


import os
import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from tqdm.notebook import tqdm

TIME_STEPS = 6

# Generated training sequences for use in the model.
def _create_sequences(values, seq_length, stride, historical=False):
    seq = []
    if historical:
        for i in range(seq_length, len(values) + 1, stride):
            seq.append(values[i-seq_length:i])
    else:
        for i in range(0, len(values) - seq_length + 1, stride):
            seq.append(values[i : i + seq_length])
   
    return np.stack(seq)


def load_tods(seq_length=0, stride=1):
    # sequence length:
    # stride: 
    # source: https://github.com/datamllab/tods/tree/benchmark
    data_path = './length6'
    datasets = sorted([f for f in os.listdir(f'{data_path}/train') if os.path.isfile(os.path.join(f'{data_path}/train', f))])

    x_train, x_test, y_test = [], [], []
    label_seq, test_seq = [], []
    for i, data in tqdm(enumerate(datasets)):
        train_df = np.array(pd.read_csv(f'{data_path}/train/{data}'))
        train_df = train_df[:, 0].astype(float)
        
        test_df = np.array(pd.read_csv(f'{data_path}/test/{data}'))
        labels = test_df[:, -1].astype(int)
        test_df = test_df[:, i].astype(float)
        
        train_df = train_df.reshape(-1, 1)
        test_df = test_df.reshape(-1, 1)

        scaler = MinMaxScaler()
        train_df = scaler.fit_transform(train_df)
        test_df = scaler.transform(test_df)
        
        if seq_length > 0:
            x_train.append(_create_sequences(train_df, seq_length, stride=stride))
            x_test.append(_create_sequences(test_df, seq_length, stride=stride))
            y_test.append(_create_sequences(labels, seq_length, stride=stride))
        else:
            x_train.append(train_df)
            x_test.append(test_df)
            y_test.append(labels)
            
        label_seq.append(labels)
        test_seq.append(test_df)            
            
    return {'x_train': x_train, 'x_test': x_test, 'y_test': y_test, 'label_seq': label_seq, 'test_seq': test_seq}


def load_tods_sequence(seq_length=0, stride=1):
    # sequence length:
    # stride: 
    # source: https://github.com/datamllab/tods/tree/benchmark    
    data_path = './'
    datasets = sorted([f for f in os.listdir(f'{data_path}/train') if os.path.isfile(os.path.join(f'{data_path}/train', f))])
    
    x_train, x_test, y_test = [], [], []
    label_seq, test_seq = [], []
    for i, data in tqdm(enumerate(datasets)):
        train_df = np.array(pd.read_csv(f'{data_path}/train/{data}'))
        train_df = train_df[:, 0].astype(float)

        test_df = np.array(pd.read_csv(f'{data_path}/test/{data}'))
        labels = test_df[:, -1].astype(int)
        test_df = test_df[:, i].astype(float)

        train_df = train_df.reshape(-1, 1)
        test_df = test_df.reshape(-1, 1)

        scaler = MinMaxScaler()
        train_df = scaler.fit_transform(train_df)
        test_df = scaler.transform(test_df)

        x_train.append(train_df)
        x_test.append(test_df)
        y_test.append(labels)
    return {'x_train': x_train, 'x_test': x_test, 'y_test': y_test}


from sklearn.metrics import auc
def _enumerate_thresholds(rec_errors, n=1000):
    # maximum value of the anomaly score for all time steps in the test data
    thresholds, step_size = [], np.max(rec_errors) / n
    th = 0.
    
    for i in range(n):
        thresholds.append(th)
        th = th + step_size
    
    return thresholds

def _compute_anomaly_scores(x, rec_x, x_val=None, scoring='square_mean'):
    if scoring == 'absolute':
        return np.mean(np.abs(rec_x - x), axis=-1)
    elif scoring == 'square_mean':
        return np.mean(np.square(rec_x - x), axis=-1) # ref. S-RNNs
    elif scoring == 'square_median':
        return np.median(np.square(rec_x - x), axis=-1)
    elif scoring == 'probability':
        return None # ref. RAMED expect to fill in

def set_thresholds(x, rec_x, is_reconstructed=True, n=1000, scoring='square_mean'):
    rec_errors = _compute_anomaly_scores(x, rec_x, scoring) if is_reconstructed else rec_x
    if len(rec_errors.shape) > 2:
        if scoring.split('_')[1] == 'mean':
            rec_errors = np.mean(rec_errors, axis=0)
        else:
            rec_errors = np.median(rec_errors, axis=0)
            
    thresholds = _enumerate_thresholds(rec_errors, n)
    return thresholds    
    
def evaluate(x, rec_x, labels, is_reconstructed=True, n=1000, scoring='square_mean', x_val=None):
    TP, TN, FP, FN = [], [], [], []
    precision, recall, f1, fpr = [], [], [], []
    
    rec_errors = _compute_anomaly_scores(x, rec_x, scoring) if is_reconstructed else rec_x
    if len(rec_errors.shape) > 2:
        if scoring.split('_')[1] == 'mean':
            rec_errors = np.mean(rec_errors, axis=0)
        else:
            rec_errors = np.median(rec_errors, axis=0)
            
    thresholds = _enumerate_thresholds(rec_errors, n)
    
    for th in thresholds: # for each threshold
        TP_t, TN_t, FP_t, FN_t = 0, 0, 0, 0
        for t in range(len(x)): # for each time window
            # if any part of the segment has an anomaly, we consider it as anomalous sequence

            true_anomalies, pred_anomalies = set(np.where(labels[t] == 1)[0]), set(np.where(rec_errors[t] > th)[0])

            if len(pred_anomalies) > 0 and len(pred_anomalies.intersection(true_anomalies)) > 0:
                # correct prediction (at least partial overlap with true anomalies)
                TP_t = TP_t + 1
            elif len(pred_anomalies) == 0 and len(true_anomalies) == 0:
                # correct rejection, no predicted anomaly on no true labels
                TN_t = TN_t + 1 
            elif len(pred_anomalies) > 0 and len(true_anomalies) == 0:
                # false alarm (i.e., predict anomalies on no true labels)
                FP_t = FP_t + 1
            elif len(pred_anomalies) == 0 and len(true_anomalies) > 0:
                # predict no anomaly when there is at least one true anomaly within the seq.
                FN_t = FN_t + 1
        
        TP.append(TP_t)
        TN.append(TN_t)
        FP.append(FP_t)
        FN.append(FN_t)
    
    for i in range(len(thresholds)):
        precision.append(TP[i] / (TP[i] + FP[i] + 0.0000001))
        recall.append(TP[i] / (TP[i] + FN[i] + 0.0000001)) # recall or true positive rate (TPR)
        fpr.append(FP[i] / (FP[i] + TN[i] + 0.0000001))
        f1.append(2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + 0.0000001))
    
    return {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'pr_auc': auc(recall, precision),
        'roc_auc': auc(fpr, recall),
        'thresholds': thresholds,
        'rec_errors': rec_errors
    }


def _elements(array):
    return array.ndim and array.size





univariate_dict = {0:'Point Global Ouliers', 1:'Point Contextual Ouliers', 2:'Collective Global Ouliers', 3:'Collective Seasonal Ouliers', 4:'Collective Trend Ouliers'}


total_scores = {'dataset': [], 'f1': [], 'pr_auc': [], 'roc_auc': [], 'th_index':[], 'predicted_index': []}


import copy
for loader in [load_tods]:
    seq_length = 25
    datasets = loader(seq_length, 1)
    x_trains, x_tests, y_tests = datasets['x_train'], datasets['x_test'], datasets['y_test']
    test_seq, label_seq = datasets['test_seq'], datasets['label_seq']
    
    per_window_idx = []
    evaluation_arrays = []
    rec_X = []
    for i in tqdm(range(len(x_trains))):
        X_train = x_trains[i]
        X_test = x_tests[i]
        
        model = GRU_AE(X_train)
        rec_x = model.predict(X_test)
        thresholds = set_thresholds(X_test, rec_x, is_reconstructed=True)
        scores = evaluate(X_test, rec_x, y_tests[i], is_reconstructed=True)
        rec_X.append(rec_x)

        total_scores['dataset'].append(f'D{i+1}')
        total_scores['f1'].append(np.max(scores['f1']))
        total_scores['pr_auc'].append(scores['pr_auc'])
        total_scores['roc_auc'].append(scores['roc_auc'])
        th_index = int(np.median(np.where(scores['f1']==np.max(scores['f1']))[0]))
        total_scores['th_index'].append(th_index)
        print(f'D{i+1}', np.max(scores['f1']), scores['pr_auc'], scores['roc_auc'])
        
        # For plotting evaluation results
        evaluation_array = np.zeros((6, len(test_seq[i])))
        predicted_normal_array = np.zeros((len(test_seq[i])))
        predicted_anomaly_array = np.zeros((len(test_seq[i])))
    
        num_context = 0
        for ts in range(len(test_seq[i])):
            if ts < seq_length - 1:
                num_context = ts + 1
            elif ts >= seq_length - 1 and ts < len(test_seq[i]) - seq_length + 1:
                num_context = seq_length
            elif ts >= len(test_seq[i]) - seq_length + 1:
                num_context = len(test_seq[i]) - ts
            evaluation_array[2][ts] = num_context
    
        pred_anomal_idx = []
        for t in range(len(X_test)):
            pred_normals = np.where(scores['rec_errors'][t] <= thresholds[th_index])[0]
            pred_anomalies = np.where(scores['rec_errors'][t] > thresholds[th_index])[0]
            
            # For Noraml
            for j in range(len(pred_normals)):
                predicted_normal_array[pred_normals[j] + t] += 1
            # For Abnormal
            for k in range(len(pred_anomalies)):
                predicted_anomaly_array[pred_anomalies[k] + t] += 1
            
            isEmpty = (_elements(pred_anomalies) == 0)
            if isEmpty:
                pass
            else:
                if pred_anomalies[0] == 0:
                    pred_anomal_idx.append(t)
        per_window_idx.append(pred_anomal_idx)
        
        evaluation_array[0] = predicted_normal_array
        evaluation_array[1] = predicted_anomaly_array
        
        # Predicted Anomaly Percentage
        for s in range(len(predicted_anomaly_array)):
            evaluation_array[3][s] = evaluation_array[1][s]/evaluation_array[2][s]
            
            # Predicted Anomaly (Binary)
            if evaluation_array[3][s] > 0.5:
                evaluation_array[4][s] = 1
        
        evaluation_array[5] = label_seq[i]
        evaluation_arrays.append(evaluation_array)
    total_scores['predicted_index'].extend(per_window_idx)
    
    # Plot
    for data_num, t in enumerate(test_seq):
        # sns.set_palette("Pastel1", 8)
        label_index = np.where(label_seq[data_num]==1)[0]
        # predict_label_index = total_scores['predicted_index'][data_num]
        predict_label_index = np.where(np.array(evaluation_arrays[data_num][4])==1)[0]

        np.save('./label_index_dynamic', label_index)
        np.save('./predicted_static', np.array(predict_label_index))

        pd.DataFrame(t).plot(figsize=(30, 8), label=None)      
        max_value = np.max(t)
        min_value = np.min(t)
        scale = (max_value-min_value)/100
        plt.vlines(label_index, max_value+2*scale, max_value+7*scale, color = 'red', linestyles='solid', label='Ground Truth')
        plt.vlines(predict_label_index, min_value-7*scale, min_value-2*scale, color = 'blue', linestyles='solid', label='Predicted')
        plt.legend(loc='upper center', ncol=3, bbox_to_anchor=(0.92, 1.07))
        plt.title(f"{data_num}_{univariate_dict[data_num]}", fontsize=15)
        plt.show()


pd.set_option('display.precision', 4)


tods_results = pd.DataFrame(total_scores)
tods_results.groupby('dataset').mean()


dfs=[]
for i in range(len(evaluation_arrays)):
    df = pd.DataFrame(evaluation_arrays[i])
    df.index = ['Normal', 'Anomaly', '#Seq', 'Pred(%)', 'Pred', 'GT']
    df = df.astype('float')
    dfs.append(df)


# Evaluation Metrics
TP, TN, FP, FN = [], [], [], []
precision, recall, f1, fpr = [], [], [], []
for data_num in range(len(dfs)):
    TP_t, TN_t, FP_t, FN_t = 0, 0, 0, 0
    for ts in dfs[data_num].columns:
        if dfs[data_num][ts]['GT'] == 1:
            if dfs[data_num][ts]['Pred'] == 1:
                TP_t = TP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                FN_t = FN_t + 1
        elif dfs[data_num][ts]['GT'] == 0:
            if dfs[data_num][ts]['Pred'] == 1:
                FP_t = FP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                TN_t = TN_t + 1

    TP.append(TP_t)
    TN.append(TN_t)
    FP.append(FP_t)
    FN.append(FN_t)
        
for i in range(len(TP)):
    precision.append(TP[i] / (TP[i] + FP[i] + 1e-8))
    recall.append(TP[i] / (TP[i] + FN[i] + 1e-8)) # recall or true positive rate (TPR)
    fpr.append(FP[i] / (FP[i] + TN[i] + 1e-8))
    f1.append(2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + 1e-8))


f1


recall


precision


pd.set_option('display.precision', 1)


dfs[0].style.background_gradient(cmap='summer', axis=1)








univariate_dict = {0:'Point Global Ouliers', 1:'Point Contextual Ouliers', 2:'Collective Global Ouliers', 3:'Collective Seasonal Ouliers', 4:'Collective Trend Ouliers'}


total_scores = {'dataset': [], 'f1': [], 'pr_auc': [], 'roc_auc': [], 'th_index':[], 'predicted_index': []}


import copy
for loader in [load_tods]:
    seq_length = 5
    datasets = loader(seq_length, 1)
    x_trains, x_tests, y_tests = datasets['x_train'], datasets['x_test'], datasets['y_test']
    test_seq, label_seq = datasets['test_seq'], datasets['label_seq']
    
    per_window_idx = []
    evaluation_arrays = []
    rec_X = []
    for i in tqdm(range(len(x_trains))):
        X_train = x_trains[i]
        X_test = x_tests[i]
        
        model = GRU_AE(X_train)
        rec_x = model.predict(X_test)
        thresholds = set_thresholds(X_test, rec_x, is_reconstructed=True)
        scores = evaluate(X_test, rec_x, y_tests[i], is_reconstructed=True)
        rec_X.append(rec_x)

        total_scores['dataset'].append(f'D{i+1}')
        total_scores['f1'].append(np.max(scores['f1']))
        total_scores['pr_auc'].append(scores['pr_auc'])
        total_scores['roc_auc'].append(scores['roc_auc'])
        th_index = int(np.median(np.where(scores['f1']==np.max(scores['f1']))[0]))
        total_scores['th_index'].append(th_index)
        print(f'D{i+1}', np.max(scores['f1']), scores['pr_auc'], scores['roc_auc'])
        
        # For plotting evaluation results
        evaluation_array = np.zeros((6, len(test_seq[i])))
        predicted_normal_array = np.zeros((len(test_seq[i])))
        predicted_anomaly_array = np.zeros((len(test_seq[i])))
    
        num_context = 0
        for ts in range(len(test_seq[i])):
            if ts < seq_length - 1:
                num_context = ts + 1
            elif ts >= seq_length - 1 and ts < len(test_seq[i]) - seq_length + 1:
                num_context = seq_length
            elif ts >= len(test_seq[i]) - seq_length + 1:
                num_context = len(test_seq[i]) - ts
            evaluation_array[2][ts] = num_context
    
        pred_anomal_idx = []
        for t in range(len(X_test)):
            pred_normals = np.where(scores['rec_errors'][t] <= thresholds[th_index])[0]
            pred_anomalies = np.where(scores['rec_errors'][t] > thresholds[th_index])[0]
            
            # For Noraml
            for j in range(len(pred_normals)):
                predicted_normal_array[pred_normals[j] + t] += 1
            # For Abnormal
            for k in range(len(pred_anomalies)):
                predicted_anomaly_array[pred_anomalies[k] + t] += 1
            
            isEmpty = (_elements(pred_anomalies) == 0)
            if isEmpty:
                pass
            else:
                if pred_anomalies[0] == 0:
                    pred_anomal_idx.append(t)
        per_window_idx.append(pred_anomal_idx)
        
        evaluation_array[0] = predicted_normal_array
        evaluation_array[1] = predicted_anomaly_array
        
        # Predicted Anomaly Percentage
        for s in range(len(predicted_anomaly_array)):
            evaluation_array[3][s] = evaluation_array[1][s]/evaluation_array[2][s]
            
            # Predicted Anomaly (Binary)
            if evaluation_array[3][s] > 0.5:
                evaluation_array[4][s] = 1
        
        evaluation_array[5] = label_seq[i]
        evaluation_arrays.append(evaluation_array)
    total_scores['predicted_index'].extend(per_window_idx)
    
    # Plot
    for data_num, t in enumerate(test_seq):
        # sns.set_palette("Pastel1", 8)
        label_index = np.where(label_seq[data_num]==1)[0]
        # predict_label_index = total_scores['predicted_index'][data_num]
        predict_label_index = np.where(np.array(evaluation_arrays[data_num][4])==1)[0]

        np.save('./label_index_dynamic', label_index)
        np.save('./predicted_static', np.array(predict_label_index))

        pd.DataFrame(t).plot(figsize=(30, 8), label=None)      
        max_value = np.max(t)
        min_value = np.min(t)
        scale = (max_value-min_value)/100
        plt.vlines(label_index, max_value+2*scale, max_value+7*scale, color = 'red', linestyles='solid', label='Ground Truth')
        plt.vlines(predict_label_index, min_value-7*scale, min_value-2*scale, color = 'blue', linestyles='solid', label='Predicted')
        plt.legend(loc='upper center', ncol=3, bbox_to_anchor=(0.92, 1.07))
        plt.title(f"{data_num}_{univariate_dict[data_num]}", fontsize=15)
        plt.show()


pd.set_option('display.precision', 4)


tods_results = pd.DataFrame(total_scores)
tods_results.groupby('dataset').mean()








dfs=[]
for i in range(len(evaluation_arrays)):
    df = pd.DataFrame(evaluation_arrays[i])
    df.index = ['Normal', 'Anomaly', '#Seq', 'Pred(%)', 'Pred', 'GT']
    df = df.astype('float')
    dfs.append(df)


# Evaluation Metrics
TP, TN, FP, FN = [], [], [], []
precision, recall, f1, fpr = [], [], [], []
for data_num in range(len(dfs)):
    TP_t, TN_t, FP_t, FN_t = 0, 0, 0, 0
    for ts in dfs[data_num].columns:
        if dfs[data_num][ts]['GT'] == 1:
            if dfs[data_num][ts]['Pred'] == 1:
                TP_t = TP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                FN_t = FN_t + 1
        elif dfs[data_num][ts]['GT'] == 0:
            if dfs[data_num][ts]['Pred'] == 1:
                FP_t = FP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                TN_t = TN_t + 1

    TP.append(TP_t)
    TN.append(TN_t)
    FP.append(FP_t)
    FN.append(FN_t)
        
for i in range(len(TP)):
    precision.append(TP[i] / (TP[i] + FP[i] + 1e-8))
    recall.append(TP[i] / (TP[i] + FN[i] + 1e-8)) # recall or true positive rate (TPR)
    fpr.append(FP[i] / (FP[i] + TN[i] + 1e-8))
    f1.append(2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + 1e-8))


f1


recall


precision


pd.set_option('display.precision', 1)


dfs[0].style.background_gradient(cmap='summer', axis=1)











univariate_dict = {0:'Point Global Ouliers', 1:'Point Contextual Ouliers', 2:'Collective Global Ouliers', 3:'Collective Seasonal Ouliers', 4:'Collective Trend Ouliers'}


total_scores = {'dataset': [], 'f1': [], 'pr_auc': [], 'roc_auc': [], 'th_index':[], 'predicted_index': []}


import copy
for loader in [load_tods]:
    seq_length = 10
    datasets = loader(seq_length, 1)
    x_trains, x_tests, y_tests = datasets['x_train'], datasets['x_test'], datasets['y_test']
    test_seq, label_seq = datasets['test_seq'], datasets['label_seq']
    
    per_window_idx = []
    evaluation_arrays = []
    rec_X = []
    for i in tqdm(range(len(x_trains))):
        X_train = x_trains[i]
        X_test = x_tests[i]
        
        model = GRU_AE(X_train)
        rec_x = model.predict(X_test)
        thresholds = set_thresholds(X_test, rec_x, is_reconstructed=True)
        scores = evaluate(X_test, rec_x, y_tests[i], is_reconstructed=True)
        rec_X.append(rec_x)

        total_scores['dataset'].append(f'D{i+1}')
        total_scores['f1'].append(np.max(scores['f1']))
        total_scores['pr_auc'].append(scores['pr_auc'])
        total_scores['roc_auc'].append(scores['roc_auc'])
        th_index = int(np.median(np.where(scores['f1']==np.max(scores['f1']))[0]))
        total_scores['th_index'].append(th_index)
        print(f'D{i+1}', np.max(scores['f1']), scores['pr_auc'], scores['roc_auc'])
        
        # For plotting evaluation results
        evaluation_array = np.zeros((6, len(test_seq[i])))
        predicted_normal_array = np.zeros((len(test_seq[i])))
        predicted_anomaly_array = np.zeros((len(test_seq[i])))
    
        num_context = 0
        for ts in range(len(test_seq[i])):
            if ts < seq_length - 1:
                num_context = ts + 1
            elif ts >= seq_length - 1 and ts < len(test_seq[i]) - seq_length + 1:
                num_context = seq_length
            elif ts >= len(test_seq[i]) - seq_length + 1:
                num_context = len(test_seq[i]) - ts
            evaluation_array[2][ts] = num_context
    
        pred_anomal_idx = []
        for t in range(len(X_test)):
            pred_normals = np.where(scores['rec_errors'][t] <= thresholds[th_index])[0]
            pred_anomalies = np.where(scores['rec_errors'][t] > thresholds[th_index])[0]
            
            # For Noraml
            for j in range(len(pred_normals)):
                predicted_normal_array[pred_normals[j] + t] += 1
            # For Abnormal
            for k in range(len(pred_anomalies)):
                predicted_anomaly_array[pred_anomalies[k] + t] += 1
            
            isEmpty = (_elements(pred_anomalies) == 0)
            if isEmpty:
                pass
            else:
                if pred_anomalies[0] == 0:
                    pred_anomal_idx.append(t)
        per_window_idx.append(pred_anomal_idx)
        
        evaluation_array[0] = predicted_normal_array
        evaluation_array[1] = predicted_anomaly_array
        
        # Predicted Anomaly Percentage
        for s in range(len(predicted_anomaly_array)):
            evaluation_array[3][s] = evaluation_array[1][s]/evaluation_array[2][s]
            
            # Predicted Anomaly (Binary)
            if evaluation_array[3][s] > 0.5:
                evaluation_array[4][s] = 1
        
        evaluation_array[5] = label_seq[i]
        evaluation_arrays.append(evaluation_array)
    total_scores['predicted_index'].extend(per_window_idx)
    
    # Plot
    for data_num, t in enumerate(test_seq):
        # sns.set_palette("Pastel1", 8)
        label_index = np.where(label_seq[data_num]==1)[0]
        # predict_label_index = total_scores['predicted_index'][data_num]
        predict_label_index = np.where(np.array(evaluation_arrays[data_num][4])==1)[0]

        np.save('./label_index_dynamic', label_index)
        np.save('./predicted_static', np.array(predict_label_index))

        pd.DataFrame(t).plot(figsize=(30, 8), label=None)      
        max_value = np.max(t)
        min_value = np.min(t)
        scale = (max_value-min_value)/100
        plt.vlines(label_index, max_value+2*scale, max_value+7*scale, color = 'red', linestyles='solid', label='Ground Truth')
        plt.vlines(predict_label_index, min_value-7*scale, min_value-2*scale, color = 'blue', linestyles='solid', label='Predicted')
        plt.legend(loc='upper center', ncol=3, bbox_to_anchor=(0.92, 1.07))
        plt.title(f"{data_num}_{univariate_dict[data_num]}", fontsize=15)
        plt.show()


pd.set_option('display.precision', 4)


tods_results = pd.DataFrame(total_scores)
tods_results.groupby('dataset').mean()











dfs=[]
for i in range(len(evaluation_arrays)):
    df = pd.DataFrame(evaluation_arrays[i])
    df.index = ['Normal', 'Anomaly', '#Seq', 'Pred(%)', 'Pred', 'GT']
    df = df.astype('float')
    dfs.append(df)


# Evaluation Metrics
TP, TN, FP, FN = [], [], [], []
precision, recall, f1, fpr = [], [], [], []
for data_num in range(len(dfs)):
    TP_t, TN_t, FP_t, FN_t = 0, 0, 0, 0
    for ts in dfs[data_num].columns:
        if dfs[data_num][ts]['GT'] == 1:
            if dfs[data_num][ts]['Pred'] == 1:
                TP_t = TP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                FN_t = FN_t + 1
        elif dfs[data_num][ts]['GT'] == 0:
            if dfs[data_num][ts]['Pred'] == 1:
                FP_t = FP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                TN_t = TN_t + 1

    TP.append(TP_t)
    TN.append(TN_t)
    FP.append(FP_t)
    FN.append(FN_t)
        
for i in range(len(TP)):
    precision.append(TP[i] / (TP[i] + FP[i] + 1e-8))
    recall.append(TP[i] / (TP[i] + FN[i] + 1e-8)) # recall or true positive rate (TPR)
    fpr.append(FP[i] / (FP[i] + TN[i] + 1e-8))
    f1.append(2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + 1e-8))


f1


recall


precision


pd.set_option('display.precision', 1)


dfs[0].style.background_gradient(cmap='summer', axis=1)








univariate_dict = {0:'Point Global Ouliers', 1:'Point Contextual Ouliers', 2:'Collective Global Ouliers', 3:'Collective Seasonal Ouliers', 4:'Collective Trend Ouliers'}


total_scores = {'dataset': [], 'f1': [], 'pr_auc': [], 'roc_auc': [], 'th_index':[], 'predicted_index': []}


import copy
for loader in [load_tods]:
    seq_length = 50
    datasets = loader(seq_length, 1)
    x_trains, x_tests, y_tests = datasets['x_train'], datasets['x_test'], datasets['y_test']
    test_seq, label_seq = datasets['test_seq'], datasets['label_seq']
    
    per_window_idx = []
    evaluation_arrays = []
    rec_X = []
    for i in tqdm(range(len(x_trains))):
        X_train = x_trains[i]
        X_test = x_tests[i]
        
        model = GRU_AE(X_train)
        rec_x = model.predict(X_test)
        thresholds = set_thresholds(X_test, rec_x, is_reconstructed=True)
        scores = evaluate(X_test, rec_x, y_tests[i], is_reconstructed=True)
        rec_X.append(rec_x)

        total_scores['dataset'].append(f'D{i+1}')
        total_scores['f1'].append(np.max(scores['f1']))
        total_scores['pr_auc'].append(scores['pr_auc'])
        total_scores['roc_auc'].append(scores['roc_auc'])
        th_index = int(np.median(np.where(scores['f1']==np.max(scores['f1']))[0]))
        total_scores['th_index'].append(th_index)
        print(f'D{i+1}', np.max(scores['f1']), scores['pr_auc'], scores['roc_auc'])
        
        # For plotting evaluation results
        evaluation_array = np.zeros((6, len(test_seq[i])))
        predicted_normal_array = np.zeros((len(test_seq[i])))
        predicted_anomaly_array = np.zeros((len(test_seq[i])))
    
        num_context = 0
        for ts in range(len(test_seq[i])):
            if ts < seq_length - 1:
                num_context = ts + 1
            elif ts >= seq_length - 1 and ts < len(test_seq[i]) - seq_length + 1:
                num_context = seq_length
            elif ts >= len(test_seq[i]) - seq_length + 1:
                num_context = len(test_seq[i]) - ts
            evaluation_array[2][ts] = num_context
    
        pred_anomal_idx = []
        for t in range(len(X_test)):
            pred_normals = np.where(scores['rec_errors'][t] <= thresholds[th_index])[0]
            pred_anomalies = np.where(scores['rec_errors'][t] > thresholds[th_index])[0]
            
            # For Noraml
            for j in range(len(pred_normals)):
                predicted_normal_array[pred_normals[j] + t] += 1
            # For Abnormal
            for k in range(len(pred_anomalies)):
                predicted_anomaly_array[pred_anomalies[k] + t] += 1
            
            isEmpty = (_elements(pred_anomalies) == 0)
            if isEmpty:
                pass
            else:
                if pred_anomalies[0] == 0:
                    pred_anomal_idx.append(t)
        per_window_idx.append(pred_anomal_idx)
        
        evaluation_array[0] = predicted_normal_array
        evaluation_array[1] = predicted_anomaly_array
        
        # Predicted Anomaly Percentage
        for s in range(len(predicted_anomaly_array)):
            evaluation_array[3][s] = evaluation_array[1][s]/evaluation_array[2][s]
            
            # Predicted Anomaly (Binary)
            if evaluation_array[3][s] > 0.5:
                evaluation_array[4][s] = 1
        
        evaluation_array[5] = label_seq[i]
        evaluation_arrays.append(evaluation_array)
    total_scores['predicted_index'].extend(per_window_idx)
    
    # Plot
    for data_num, t in enumerate(test_seq):
        # sns.set_palette("Pastel1", 8)
        label_index = np.where(label_seq[data_num]==1)[0]
        # predict_label_index = total_scores['predicted_index'][data_num]
        predict_label_index = np.where(np.array(evaluation_arrays[data_num][4])==1)[0]

        np.save('./label_index_dynamic', label_index)
        np.save('./predicted_static', np.array(predict_label_index))

        pd.DataFrame(t).plot(figsize=(30, 8), label=None)      
        max_value = np.max(t)
        min_value = np.min(t)
        scale = (max_value-min_value)/100
        plt.vlines(label_index, max_value+2*scale, max_value+7*scale, color = 'red', linestyles='solid', label='Ground Truth')
        plt.vlines(predict_label_index, min_value-7*scale, min_value-2*scale, color = 'blue', linestyles='solid', label='Predicted')
        plt.legend(loc='upper center', ncol=3, bbox_to_anchor=(0.92, 1.07))
        plt.title(f"{data_num}_{univariate_dict[data_num]}", fontsize=15)
        plt.show()


pd.set_option('display.precision', 4)


tods_results = pd.DataFrame(total_scores)
tods_results.groupby('dataset').mean()











dfs=[]
for i in range(len(evaluation_arrays)):
    df = pd.DataFrame(evaluation_arrays[i])
    df.index = ['Normal', 'Anomaly', '#Seq', 'Pred(%)', 'Pred', 'GT']
    df = df.astype('float')
    dfs.append(df)


# Evaluation Metrics
TP, TN, FP, FN = [], [], [], []
precision, recall, f1, fpr = [], [], [], []
for data_num in range(len(dfs)):
    TP_t, TN_t, FP_t, FN_t = 0, 0, 0, 0
    for ts in dfs[data_num].columns:
        if dfs[data_num][ts]['GT'] == 1:
            if dfs[data_num][ts]['Pred'] == 1:
                TP_t = TP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                FN_t = FN_t + 1
        elif dfs[data_num][ts]['GT'] == 0:
            if dfs[data_num][ts]['Pred'] == 1:
                FP_t = FP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                TN_t = TN_t + 1

    TP.append(TP_t)
    TN.append(TN_t)
    FP.append(FP_t)
    FN.append(FN_t)
        
for i in range(len(TP)):
    precision.append(TP[i] / (TP[i] + FP[i] + 1e-8))
    recall.append(TP[i] / (TP[i] + FN[i] + 1e-8)) # recall or true positive rate (TPR)
    fpr.append(FP[i] / (FP[i] + TN[i] + 1e-8))
    f1.append(2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + 1e-8))


f1


recall


precision


pd.set_option('display.precision', 1)


dfs[0].style.background_gradient(cmap='summer', axis=1)








univariate_dict = {0:'Point Global Ouliers', 1:'Point Contextual Ouliers', 2:'Collective Global Ouliers', 3:'Collective Seasonal Ouliers', 4:'Collective Trend Ouliers'}


total_scores = {'dataset': [], 'f1': [], 'pr_auc': [], 'roc_auc': [], 'th_index':[], 'predicted_index': []}


import copy
for loader in [load_tods]:
    seq_length = 30
    datasets = loader(seq_length, 1)
    x_trains, x_tests, y_tests = datasets['x_train'], datasets['x_test'], datasets['y_test']
    test_seq, label_seq = datasets['test_seq'], datasets['label_seq']
    
    per_window_idx = []
    evaluation_arrays = []
    rec_X = []
    for i in tqdm(range(len(x_trains))):
        X_train = x_trains[i]
        X_test = x_tests[i]
        
        model = GRU_AE(X_train)
        rec_x = model.predict(X_test)
        thresholds = set_thresholds(X_test, rec_x, is_reconstructed=True)
        scores = evaluate(X_test, rec_x, y_tests[i], is_reconstructed=True)
        rec_X.append(rec_x)

        total_scores['dataset'].append(f'D{i+1}')
        total_scores['f1'].append(np.max(scores['f1']))
        total_scores['pr_auc'].append(scores['pr_auc'])
        total_scores['roc_auc'].append(scores['roc_auc'])
        th_index = int(np.median(np.where(scores['f1']==np.max(scores['f1']))[0]))
        total_scores['th_index'].append(th_index)
        print(f'D{i+1}', np.max(scores['f1']), scores['pr_auc'], scores['roc_auc'])
        
        # For plotting evaluation results
        evaluation_array = np.zeros((6, len(test_seq[i])))
        predicted_normal_array = np.zeros((len(test_seq[i])))
        predicted_anomaly_array = np.zeros((len(test_seq[i])))
    
        num_context = 0
        for ts in range(len(test_seq[i])):
            if ts < seq_length - 1:
                num_context = ts + 1
            elif ts >= seq_length - 1 and ts < len(test_seq[i]) - seq_length + 1:
                num_context = seq_length
            elif ts >= len(test_seq[i]) - seq_length + 1:
                num_context = len(test_seq[i]) - ts
            evaluation_array[2][ts] = num_context
    
        pred_anomal_idx = []
        for t in range(len(X_test)):
            pred_normals = np.where(scores['rec_errors'][t] <= thresholds[th_index])[0]
            pred_anomalies = np.where(scores['rec_errors'][t] > thresholds[th_index])[0]
            
            # For Noraml
            for j in range(len(pred_normals)):
                predicted_normal_array[pred_normals[j] + t] += 1
            # For Abnormal
            for k in range(len(pred_anomalies)):
                predicted_anomaly_array[pred_anomalies[k] + t] += 1
            
            isEmpty = (_elements(pred_anomalies) == 0)
            if isEmpty:
                pass
            else:
                if pred_anomalies[0] == 0:
                    pred_anomal_idx.append(t)
        per_window_idx.append(pred_anomal_idx)
        
        evaluation_array[0] = predicted_normal_array
        evaluation_array[1] = predicted_anomaly_array
        
        # Predicted Anomaly Percentage
        for s in range(len(predicted_anomaly_array)):
            evaluation_array[3][s] = evaluation_array[1][s]/evaluation_array[2][s]
            
            # Predicted Anomaly (Binary)
            if evaluation_array[3][s] > 0.5:
                evaluation_array[4][s] = 1
        
        evaluation_array[5] = label_seq[i]
        evaluation_arrays.append(evaluation_array)
    total_scores['predicted_index'].extend(per_window_idx)
    
    # Plot
    for data_num, t in enumerate(test_seq):
        # sns.set_palette("Pastel1", 8)
        label_index = np.where(label_seq[data_num]==1)[0]
        # predict_label_index = total_scores['predicted_index'][data_num]
        predict_label_index = np.where(np.array(evaluation_arrays[data_num][4])==1)[0]

        np.save('./label_index_dynamic', label_index)
        np.save('./predicted_static', np.array(predict_label_index))

        pd.DataFrame(t).plot(figsize=(30, 8), label=None)      
        max_value = np.max(t)
        min_value = np.min(t)
        scale = (max_value-min_value)/100
        plt.vlines(label_index, max_value+2*scale, max_value+7*scale, color = 'red', linestyles='solid', label='Ground Truth')
        plt.vlines(predict_label_index, min_value-7*scale, min_value-2*scale, color = 'blue', linestyles='solid', label='Predicted')
        plt.legend(loc='upper center', ncol=3, bbox_to_anchor=(0.92, 1.07))
        plt.title(f"{data_num}_{univariate_dict[data_num]}", fontsize=15)
        plt.show()


pd.set_option('display.precision', 4)


tods_results = pd.DataFrame(total_scores)
tods_results.groupby('dataset').mean()


sns.set(style='darkgrid')
ncols = 5
row = 0
col = 0

x_test = x_tests[0]
rec_x = rec_X[0]
fig, ax = plt.subplots(ncols=ncols, nrows=len(x_test)//ncols + 1, figsize=(20, 200))
for i in range(len(x_test)):
    ax[row][col].plot(np.squeeze(x_test[i]), label='raw')
    ax[row][col].plot(np.squeeze(rec_x[i]), label='pred')
    col += 1
    if (i != 0) & ((i+1) % ncols == 0):
        row += 1
        col = 0
    ax[row][col].legend(['raw','pred'])











dfs=[]
for i in range(len(evaluation_arrays)):
    df = pd.DataFrame(evaluation_arrays[i])
    df.index = ['Normal', 'Anomaly', '#Seq', 'Pred(%)', 'Pred', 'GT']
    df = df.astype('float')
    dfs.append(df)


# Evaluation Metrics
TP, TN, FP, FN = [], [], [], []
precision, recall, f1, fpr = [], [], [], []
for data_num in range(len(dfs)):
    TP_t, TN_t, FP_t, FN_t = 0, 0, 0, 0
    for ts in dfs[data_num].columns:
        if dfs[data_num][ts]['GT'] == 1:
            if dfs[data_num][ts]['Pred'] == 1:
                TP_t = TP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                FN_t = FN_t + 1
        elif dfs[data_num][ts]['GT'] == 0:
            if dfs[data_num][ts]['Pred'] == 1:
                FP_t = FP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                TN_t = TN_t + 1

    TP.append(TP_t)
    TN.append(TN_t)
    FP.append(FP_t)
    FN.append(FN_t)
        
for i in range(len(TP)):
    precision.append(TP[i] / (TP[i] + FP[i] + 1e-8))
    recall.append(TP[i] / (TP[i] + FN[i] + 1e-8)) # recall or true positive rate (TPR)
    fpr.append(FP[i] / (FP[i] + TN[i] + 1e-8))
    f1.append(2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + 1e-8))


f1


recall


precision


pd.set_option('display.precision', 1)


dfs[0].style.background_gradient(cmap='summer', axis=1)











univariate_dict = {0:'Point Global Ouliers', 1:'Point Contextual Ouliers', 2:'Collective Global Ouliers', 3:'Collective Seasonal Ouliers', 4:'Collective Trend Ouliers'}


total_scores = {'dataset': [], 'f1': [], 'pr_auc': [], 'roc_auc': [], 'th_index':[], 'predicted_index': []}


import copy
for loader in [load_tods]:
    seq_length = 40
    datasets = loader(seq_length, 1)
    x_trains, x_tests, y_tests = datasets['x_train'], datasets['x_test'], datasets['y_test']
    test_seq, label_seq = datasets['test_seq'], datasets['label_seq']
    
    per_window_idx = []
    evaluation_arrays = []
    rec_X = []
    for i in tqdm(range(len(x_trains))):
        X_train = x_trains[i]
        X_test = x_tests[i]
        
        model = GRU_AE(X_train)
        rec_x = model.predict(X_test)
        thresholds = set_thresholds(X_test, rec_x, is_reconstructed=True)
        scores = evaluate(X_test, rec_x, y_tests[i], is_reconstructed=True)
        rec_X.append(rec_x)

        total_scores['dataset'].append(f'D{i+1}')
        total_scores['f1'].append(np.max(scores['f1']))
        total_scores['pr_auc'].append(scores['pr_auc'])
        total_scores['roc_auc'].append(scores['roc_auc'])
        th_index = int(np.median(np.where(scores['f1']==np.max(scores['f1']))[0]))
        total_scores['th_index'].append(th_index)
        print(f'D{i+1}', np.max(scores['f1']), scores['pr_auc'], scores['roc_auc'])
        
        # For plotting evaluation results
        evaluation_array = np.zeros((6, len(test_seq[i])))
        predicted_normal_array = np.zeros((len(test_seq[i])))
        predicted_anomaly_array = np.zeros((len(test_seq[i])))
    
        num_context = 0
        for ts in range(len(test_seq[i])):
            if ts < seq_length - 1:
                num_context = ts + 1
            elif ts >= seq_length - 1 and ts < len(test_seq[i]) - seq_length + 1:
                num_context = seq_length
            elif ts >= len(test_seq[i]) - seq_length + 1:
                num_context = len(test_seq[i]) - ts
            evaluation_array[2][ts] = num_context
    
        pred_anomal_idx = []
        for t in range(len(X_test)):
            pred_normals = np.where(scores['rec_errors'][t] <= thresholds[th_index])[0]
            pred_anomalies = np.where(scores['rec_errors'][t] > thresholds[th_index])[0]
            
            # For Noraml
            for j in range(len(pred_normals)):
                predicted_normal_array[pred_normals[j] + t] += 1
            # For Abnormal
            for k in range(len(pred_anomalies)):
                predicted_anomaly_array[pred_anomalies[k] + t] += 1
            
            isEmpty = (_elements(pred_anomalies) == 0)
            if isEmpty:
                pass
            else:
                if pred_anomalies[0] == 0:
                    pred_anomal_idx.append(t)
        per_window_idx.append(pred_anomal_idx)
        
        evaluation_array[0] = predicted_normal_array
        evaluation_array[1] = predicted_anomaly_array
        
        # Predicted Anomaly Percentage
        for s in range(len(predicted_anomaly_array)):
            evaluation_array[3][s] = evaluation_array[1][s]/evaluation_array[2][s]
            
            # Predicted Anomaly (Binary)
            if evaluation_array[3][s] > 0.5:
                evaluation_array[4][s] = 1
        
        evaluation_array[5] = label_seq[i]
        evaluation_arrays.append(evaluation_array)
    total_scores['predicted_index'].extend(per_window_idx)
    
    # Plot
    for data_num, t in enumerate(test_seq):
        # sns.set_palette("Pastel1", 8)
        label_index = np.where(label_seq[data_num]==1)[0]
        # predict_label_index = total_scores['predicted_index'][data_num]
        predict_label_index = np.where(np.array(evaluation_arrays[data_num][4])==1)[0]

        np.save('./label_index_dynamic', label_index)
        np.save('./predicted_static', np.array(predict_label_index))

        pd.DataFrame(t).plot(figsize=(30, 8), label=None)      
        max_value = np.max(t)
        min_value = np.min(t)
        scale = (max_value-min_value)/100
        plt.vlines(label_index, max_value+2*scale, max_value+7*scale, color = 'red', linestyles='solid', label='Ground Truth')
        plt.vlines(predict_label_index, min_value-7*scale, min_value-2*scale, color = 'blue', linestyles='solid', label='Predicted')
        plt.legend(loc='upper center', ncol=3, bbox_to_anchor=(0.92, 1.07))
        plt.title(f"{data_num}_{univariate_dict[data_num]}", fontsize=15)
        plt.show()


pd.set_option('display.precision', 4)


tods_results = pd.DataFrame(total_scores)
tods_results.groupby('dataset').mean()











dfs=[]
for i in range(len(evaluation_arrays)):
    df = pd.DataFrame(evaluation_arrays[i])
    df.index = ['Normal', 'Anomaly', '#Seq', 'Pred(%)', 'Pred', 'GT']
    df = df.astype('float')
    dfs.append(df)


# Evaluation Metrics
TP, TN, FP, FN = [], [], [], []
precision, recall, f1, fpr = [], [], [], []
for data_num in range(len(dfs)):
    TP_t, TN_t, FP_t, FN_t = 0, 0, 0, 0
    for ts in dfs[data_num].columns:
        if dfs[data_num][ts]['GT'] == 1:
            if dfs[data_num][ts]['Pred'] == 1:
                TP_t = TP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                FN_t = FN_t + 1
        elif dfs[data_num][ts]['GT'] == 0:
            if dfs[data_num][ts]['Pred'] == 1:
                FP_t = FP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                TN_t = TN_t + 1

    TP.append(TP_t)
    TN.append(TN_t)
    FP.append(FP_t)
    FN.append(FN_t)
        
for i in range(len(TP)):
    precision.append(TP[i] / (TP[i] + FP[i] + 1e-8))
    recall.append(TP[i] / (TP[i] + FN[i] + 1e-8)) # recall or true positive rate (TPR)
    fpr.append(FP[i] / (FP[i] + TN[i] + 1e-8))
    f1.append(2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + 1e-8))


f1


recall


precision


pd.set_option('display.precision', 1)


dfs[0].style.background_gradient(cmap='summer', axis=1)








univariate_dict = {0:'Point Global Ouliers', 1:'Point Contextual Ouliers', 2:'Collective Global Ouliers', 3:'Collective Seasonal Ouliers', 4:'Collective Trend Ouliers'}


total_scores = {'dataset': [], 'f1': [], 'pr_auc': [], 'roc_auc': [], 'th_index':[], 'predicted_index': []}


import copy
for loader in [load_tods]:
    seq_length = 75
    datasets = loader(seq_length, 1)
    x_trains, x_tests, y_tests = datasets['x_train'], datasets['x_test'], datasets['y_test']
    test_seq, label_seq = datasets['test_seq'], datasets['label_seq']
    
    per_window_idx = []
    evaluation_arrays = []
    rec_X = []
    for i in tqdm(range(len(x_trains))):
        X_train = x_trains[i]
        X_test = x_tests[i]
        
        model = GRU_AE(X_train)
        rec_x = model.predict(X_test)
        thresholds = set_thresholds(X_test, rec_x, is_reconstructed=True)
        scores = evaluate(X_test, rec_x, y_tests[i], is_reconstructed=True)
        rec_X.append(rec_x)

        total_scores['dataset'].append(f'D{i+1}')
        total_scores['f1'].append(np.max(scores['f1']))
        total_scores['pr_auc'].append(scores['pr_auc'])
        total_scores['roc_auc'].append(scores['roc_auc'])
        th_index = int(np.median(np.where(scores['f1']==np.max(scores['f1']))[0]))
        total_scores['th_index'].append(th_index)
        print(f'D{i+1}', np.max(scores['f1']), scores['pr_auc'], scores['roc_auc'])
        
        # For plotting evaluation results
        evaluation_array = np.zeros((6, len(test_seq[i])))
        predicted_normal_array = np.zeros((len(test_seq[i])))
        predicted_anomaly_array = np.zeros((len(test_seq[i])))
    
        num_context = 0
        for ts in range(len(test_seq[i])):
            if ts < seq_length - 1:
                num_context = ts + 1
            elif ts >= seq_length - 1 and ts < len(test_seq[i]) - seq_length + 1:
                num_context = seq_length
            elif ts >= len(test_seq[i]) - seq_length + 1:
                num_context = len(test_seq[i]) - ts
            evaluation_array[2][ts] = num_context
    
        pred_anomal_idx = []
        for t in range(len(X_test)):
            pred_normals = np.where(scores['rec_errors'][t] <= thresholds[th_index])[0]
            pred_anomalies = np.where(scores['rec_errors'][t] > thresholds[th_index])[0]
            
            # For Noraml
            for j in range(len(pred_normals)):
                predicted_normal_array[pred_normals[j] + t] += 1
            # For Abnormal
            for k in range(len(pred_anomalies)):
                predicted_anomaly_array[pred_anomalies[k] + t] += 1
            
            isEmpty = (_elements(pred_anomalies) == 0)
            if isEmpty:
                pass
            else:
                if pred_anomalies[0] == 0:
                    pred_anomal_idx.append(t)
        per_window_idx.append(pred_anomal_idx)
        
        evaluation_array[0] = predicted_normal_array
        evaluation_array[1] = predicted_anomaly_array
        
        # Predicted Anomaly Percentage
        for s in range(len(predicted_anomaly_array)):
            evaluation_array[3][s] = evaluation_array[1][s]/evaluation_array[2][s]
            
            # Predicted Anomaly (Binary)
            if evaluation_array[3][s] > 0.5:
                evaluation_array[4][s] = 1
        
        evaluation_array[5] = label_seq[i]
        evaluation_arrays.append(evaluation_array)
    total_scores['predicted_index'].extend(per_window_idx)
    
    # Plot
    for data_num, t in enumerate(test_seq):
        # sns.set_palette("Pastel1", 8)
        label_index = np.where(label_seq[data_num]==1)[0]
        # predict_label_index = total_scores['predicted_index'][data_num]
        predict_label_index = np.where(np.array(evaluation_arrays[data_num][4])==1)[0]

        np.save('./label_index_dynamic', label_index)
        np.save('./predicted_static', np.array(predict_label_index))

        pd.DataFrame(t).plot(figsize=(30, 8), label=None)      
        max_value = np.max(t)
        min_value = np.min(t)
        scale = (max_value-min_value)/100
        plt.vlines(label_index, max_value+2*scale, max_value+7*scale, color = 'red', linestyles='solid', label='Ground Truth')
        plt.vlines(predict_label_index, min_value-7*scale, min_value-2*scale, color = 'blue', linestyles='solid', label='Predicted')
        plt.legend(loc='upper center', ncol=3, bbox_to_anchor=(0.92, 1.07))
        plt.title(f"{data_num}_{univariate_dict[data_num]}", fontsize=15)
        plt.show()


pd.set_option('display.precision', 4)


tods_results = pd.DataFrame(total_scores)
tods_results.groupby('dataset').mean()





dfs=[]
for i in range(len(evaluation_arrays)):
    df = pd.DataFrame(evaluation_arrays[i])
    df.index = ['Normal', 'Anomaly', '#Seq', 'Pred(%)', 'Pred', 'GT']
    df = df.astype('float')
    dfs.append(df)


# Evaluation Metrics
TP, TN, FP, FN = [], [], [], []
precision, recall, f1, fpr = [], [], [], []
for data_num in range(len(dfs)):
    TP_t, TN_t, FP_t, FN_t = 0, 0, 0, 0
    for ts in dfs[data_num].columns:
        if dfs[data_num][ts]['GT'] == 1:
            if dfs[data_num][ts]['Pred'] == 1:
                TP_t = TP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                FN_t = FN_t + 1
        elif dfs[data_num][ts]['GT'] == 0:
            if dfs[data_num][ts]['Pred'] == 1:
                FP_t = FP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                TN_t = TN_t + 1

    TP.append(TP_t)
    TN.append(TN_t)
    FP.append(FP_t)
    FN.append(FN_t)
        
for i in range(len(TP)):
    precision.append(TP[i] / (TP[i] + FP[i] + 1e-8))
    recall.append(TP[i] / (TP[i] + FN[i] + 1e-8)) # recall or true positive rate (TPR)
    fpr.append(FP[i] / (FP[i] + TN[i] + 1e-8))
    f1.append(2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + 1e-8))


f1


recall


precision


pd.set_option('display.precision', 1)


dfs[0].style.background_gradient(cmap='summer', axis=1)





univariate_dict = {0:'Point Global Ouliers', 1:'Point Contextual Ouliers', 2:'Collective Global Ouliers', 3:'Collective Seasonal Ouliers', 4:'Collective Trend Ouliers'}


total_scores = {'dataset': [], 'f1': [], 'pr_auc': [], 'roc_auc': [], 'th_index':[], 'predicted_index': []}


import copy
for loader in [load_tods]:
    seq_length = 100
    datasets = loader(seq_length, 1)
    x_trains, x_tests, y_tests = datasets['x_train'], datasets['x_test'], datasets['y_test']
    test_seq, label_seq = datasets['test_seq'], datasets['label_seq']
    
    per_window_idx = []
    evaluation_arrays = []
    rec_X = []
    for i in tqdm(range(len(x_trains))):
        X_train = x_trains[i]
        X_test = x_tests[i]
        
        model = GRU_AE(X_train)
        rec_x = model.predict(X_test)
        thresholds = set_thresholds(X_test, rec_x, is_reconstructed=True)
        scores = evaluate(X_test, rec_x, y_tests[i], is_reconstructed=True)
        rec_X.append(rec_x)

        total_scores['dataset'].append(f'D{i+1}')
        total_scores['f1'].append(np.max(scores['f1']))
        total_scores['pr_auc'].append(scores['pr_auc'])
        total_scores['roc_auc'].append(scores['roc_auc'])
        th_index = int(np.median(np.where(scores['f1']==np.max(scores['f1']))[0]))
        total_scores['th_index'].append(th_index)
        print(f'D{i+1}', np.max(scores['f1']), scores['pr_auc'], scores['roc_auc'])
        
        # For plotting evaluation results
        evaluation_array = np.zeros((6, len(test_seq[i])))
        predicted_normal_array = np.zeros((len(test_seq[i])))
        predicted_anomaly_array = np.zeros((len(test_seq[i])))
    
        num_context = 0
        for ts in range(len(test_seq[i])):
            if ts < seq_length - 1:
                num_context = ts + 1
            elif ts >= seq_length - 1 and ts < len(test_seq[i]) - seq_length + 1:
                num_context = seq_length
            elif ts >= len(test_seq[i]) - seq_length + 1:
                num_context = len(test_seq[i]) - ts
            evaluation_array[2][ts] = num_context
    
        pred_anomal_idx = []
        for t in range(len(X_test)):
            pred_normals = np.where(scores['rec_errors'][t] <= thresholds[th_index])[0]
            pred_anomalies = np.where(scores['rec_errors'][t] > thresholds[th_index])[0]
            
            # For Noraml
            for j in range(len(pred_normals)):
                predicted_normal_array[pred_normals[j] + t] += 1
            # For Abnormal
            for k in range(len(pred_anomalies)):
                predicted_anomaly_array[pred_anomalies[k] + t] += 1
            
            isEmpty = (_elements(pred_anomalies) == 0)
            if isEmpty:
                pass
            else:
                if pred_anomalies[0] == 0:
                    pred_anomal_idx.append(t)
        per_window_idx.append(pred_anomal_idx)
        
        evaluation_array[0] = predicted_normal_array
        evaluation_array[1] = predicted_anomaly_array
        
        # Predicted Anomaly Percentage
        for s in range(len(predicted_anomaly_array)):
            evaluation_array[3][s] = evaluation_array[1][s]/evaluation_array[2][s]
            
            # Predicted Anomaly (Binary)
            if evaluation_array[3][s] > 0.5:
                evaluation_array[4][s] = 1
        
        evaluation_array[5] = label_seq[i]
        evaluation_arrays.append(evaluation_array)
    total_scores['predicted_index'].extend(per_window_idx)
    
    # Plot
    for data_num, t in enumerate(test_seq):
        # sns.set_palette("Pastel1", 8)
        label_index = np.where(label_seq[data_num]==1)[0]
        # predict_label_index = total_scores['predicted_index'][data_num]
        predict_label_index = np.where(np.array(evaluation_arrays[data_num][4])==1)[0]

        np.save('./label_index_dynamic', label_index)
        np.save('./predicted_static', np.array(predict_label_index))

        pd.DataFrame(t).plot(figsize=(30, 8), label=None)      
        max_value = np.max(t)
        min_value = np.min(t)
        scale = (max_value-min_value)/100
        plt.vlines(label_index, max_value+2*scale, max_value+7*scale, color = 'red', linestyles='solid', label='Ground Truth')
        plt.vlines(predict_label_index, min_value-7*scale, min_value-2*scale, color = 'blue', linestyles='solid', label='Predicted')
        plt.legend(loc='upper center', ncol=3, bbox_to_anchor=(0.92, 1.07))
        plt.title(f"{data_num}_{univariate_dict[data_num]}", fontsize=15)
        plt.show()


pd.set_option('display.precision', 4)


tods_results = pd.DataFrame(total_scores)
tods_results.groupby('dataset').mean()











dfs=[]
for i in range(len(evaluation_arrays)):
    df = pd.DataFrame(evaluation_arrays[i])
    df.index = ['Normal', 'Anomaly', '#Seq', 'Pred(%)', 'Pred', 'GT']
    df = df.astype('float')
    dfs.append(df)


# Evaluation Metrics
TP, TN, FP, FN = [], [], [], []
precision, recall, f1, fpr = [], [], [], []
for data_num in range(len(dfs)):
    TP_t, TN_t, FP_t, FN_t = 0, 0, 0, 0
    for ts in dfs[data_num].columns:
        if dfs[data_num][ts]['GT'] == 1:
            if dfs[data_num][ts]['Pred'] == 1:
                TP_t = TP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                FN_t = FN_t + 1
        elif dfs[data_num][ts]['GT'] == 0:
            if dfs[data_num][ts]['Pred'] == 1:
                FP_t = FP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                TN_t = TN_t + 1

    TP.append(TP_t)
    TN.append(TN_t)
    FP.append(FP_t)
    FN.append(FN_t)
        
for i in range(len(TP)):
    precision.append(TP[i] / (TP[i] + FP[i] + 1e-8))
    recall.append(TP[i] / (TP[i] + FN[i] + 1e-8)) # recall or true positive rate (TPR)
    fpr.append(FP[i] / (FP[i] + TN[i] + 1e-8))
    f1.append(2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + 1e-8))


f1


recall


precision


pd.set_option('display.precision', 1)


dfs[0].style.background_gradient(cmap='summer', axis=1)








univariate_dict = {0:'Point Global Ouliers', 1:'Point Contextual Ouliers', 2:'Collective Global Ouliers', 3:'Collective Seasonal Ouliers', 4:'Collective Trend Ouliers'}


total_scores = {'dataset': [], 'f1': [], 'pr_auc': [], 'roc_auc': [], 'th_index':[], 'predicted_index': []}


import copy
for loader in [load_tods]:
    seq_length = 200
    datasets = loader(seq_length, 1)
    x_trains, x_tests, y_tests = datasets['x_train'], datasets['x_test'], datasets['y_test']
    test_seq, label_seq = datasets['test_seq'], datasets['label_seq']
    
    per_window_idx = []
    evaluation_arrays = []
    rec_X = []
    for i in tqdm(range(len(x_trains))):
        X_train = x_trains[i]
        X_test = x_tests[i]
        
        model = GRU_AE(X_train)
        rec_x = model.predict(X_test)
        thresholds = set_thresholds(X_test, rec_x, is_reconstructed=True)
        scores = evaluate(X_test, rec_x, y_tests[i], is_reconstructed=True)
        rec_X.append(rec_x)

        total_scores['dataset'].append(f'D{i+1}')
        total_scores['f1'].append(np.max(scores['f1']))
        total_scores['pr_auc'].append(scores['pr_auc'])
        total_scores['roc_auc'].append(scores['roc_auc'])
        th_index = int(np.median(np.where(scores['f1']==np.max(scores['f1']))[0]))
        total_scores['th_index'].append(th_index)
        print(f'D{i+1}', np.max(scores['f1']), scores['pr_auc'], scores['roc_auc'])
        
        # For plotting evaluation results
        evaluation_array = np.zeros((6, len(test_seq[i])))
        predicted_normal_array = np.zeros((len(test_seq[i])))
        predicted_anomaly_array = np.zeros((len(test_seq[i])))
    
        num_context = 0
        for ts in range(len(test_seq[i])):
            if ts < seq_length - 1:
                num_context = ts + 1
            elif ts >= seq_length - 1 and ts < len(test_seq[i]) - seq_length + 1:
                num_context = seq_length
            elif ts >= len(test_seq[i]) - seq_length + 1:
                num_context = len(test_seq[i]) - ts
            evaluation_array[2][ts] = num_context
    
        pred_anomal_idx = []
        for t in range(len(X_test)):
            pred_normals = np.where(scores['rec_errors'][t] <= thresholds[th_index])[0]
            pred_anomalies = np.where(scores['rec_errors'][t] > thresholds[th_index])[0]
            
            # For Noraml
            for j in range(len(pred_normals)):
                predicted_normal_array[pred_normals[j] + t] += 1
            # For Abnormal
            for k in range(len(pred_anomalies)):
                predicted_anomaly_array[pred_anomalies[k] + t] += 1
            
            isEmpty = (_elements(pred_anomalies) == 0)
            if isEmpty:
                pass
            else:
                if pred_anomalies[0] == 0:
                    pred_anomal_idx.append(t)
        per_window_idx.append(pred_anomal_idx)
        
        evaluation_array[0] = predicted_normal_array
        evaluation_array[1] = predicted_anomaly_array
        
        # Predicted Anomaly Percentage
        for s in range(len(predicted_anomaly_array)):
            evaluation_array[3][s] = evaluation_array[1][s]/evaluation_array[2][s]
            
            # Predicted Anomaly (Binary)
            if evaluation_array[3][s] > 0.5:
                evaluation_array[4][s] = 1
        
        evaluation_array[5] = label_seq[i]
        evaluation_arrays.append(evaluation_array)
    total_scores['predicted_index'].extend(per_window_idx)
    
    # Plot
    for data_num, t in enumerate(test_seq):
        # sns.set_palette("Pastel1", 8)
        label_index = np.where(label_seq[data_num]==1)[0]
        # predict_label_index = total_scores['predicted_index'][data_num]
        predict_label_index = np.where(np.array(evaluation_arrays[data_num][4])==1)[0]

        np.save('./label_index_dynamic', label_index)
        np.save('./predicted_static', np.array(predict_label_index))

        pd.DataFrame(t).plot(figsize=(30, 8), label=None)      
        max_value = np.max(t)
        min_value = np.min(t)
        scale = (max_value-min_value)/100
        plt.vlines(label_index, max_value+2*scale, max_value+7*scale, color = 'red', linestyles='solid', label='Ground Truth')
        plt.vlines(predict_label_index, min_value-7*scale, min_value-2*scale, color = 'blue', linestyles='solid', label='Predicted')
        plt.legend(loc='upper center', ncol=3, bbox_to_anchor=(0.92, 1.07))
        plt.title(f"{data_num}_{univariate_dict[data_num]}", fontsize=15)
        plt.show()


pd.set_option('display.precision', 4)


tods_results = pd.DataFrame(total_scores)
tods_results.groupby('dataset').mean()











dfs=[]
for i in range(len(evaluation_arrays)):
    df = pd.DataFrame(evaluation_arrays[i])
    df.index = ['Normal', 'Anomaly', '#Seq', 'Pred(%)', 'Pred', 'GT']
    df = df.astype('float')
    dfs.append(df)


# Evaluation Metrics
TP, TN, FP, FN = [], [], [], []
precision, recall, f1, fpr = [], [], [], []
for data_num in range(len(dfs)):
    TP_t, TN_t, FP_t, FN_t = 0, 0, 0, 0
    for ts in dfs[data_num].columns:
        if dfs[data_num][ts]['GT'] == 1:
            if dfs[data_num][ts]['Pred'] == 1:
                TP_t = TP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                FN_t = FN_t + 1
        elif dfs[data_num][ts]['GT'] == 0:
            if dfs[data_num][ts]['Pred'] == 1:
                FP_t = FP_t + 1
            elif dfs[data_num][ts]['Pred'] == 0:
                TN_t = TN_t + 1

    TP.append(TP_t)
    TN.append(TN_t)
    FP.append(FP_t)
    FN.append(FN_t)
        
for i in range(len(TP)):
    precision.append(TP[i] / (TP[i] + FP[i] + 1e-8))
    recall.append(TP[i] / (TP[i] + FN[i] + 1e-8)) # recall or true positive rate (TPR)
    fpr.append(FP[i] / (FP[i] + TN[i] + 1e-8))
    f1.append(2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + 1e-8))


f1


recall


precision


pd.set_option('display.precision', 1)


dfs[0].style.background_gradient(cmap='summer', axis=1)






