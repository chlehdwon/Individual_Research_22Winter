import json
import os
import random

import numpy as np
import pandas as pd
import sklearn
from celluloid import Camera
from matplotlib import pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers
from tqdm.notebook import tqdm
from evaluator import evaluate, compute_threshold
from dataloader import loader

# from sklearn.metrics import confusion_matrix

os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
os.environ["TF_FORCE_GPU_ALLOW_GROWTH"] = "true"


df_train, df_test, df_test_label = loader(dataset="UCR")
print(df_train.shape)


fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True, sharey=True)
df_train.plot(legend=False, ax=ax, subplots=True)
plt.show()


def plot_sensor(df_test, df_test_label, save_path="./UCR.gif"):
    fig = plt.figure(figsize=(16, 6))
    ## 에니메이션 만들기
    camera = Camera(fig)
    ax = fig.add_subplot(111)

    ## 불량 구간 탐색 데이터
    labels = df_test_label["label"].values.tolist()
    idxs = df_test.index

    for var_name in tqdm([item for item in df_test.columns if "feature" in item]):
        ## 센서별로 사진 찍기
        df_test[var_name].plot(ax=ax)
        ax.legend([var_name], loc="upper right")

        ## 고장구간 표시
        temp_start = idxs[0]
        temp_date = idxs[0]
        temp_label = labels[0]

        for xc, value in zip(idxs, labels):
            if temp_label != value:
                if temp_label == True:
                    ax.axvspan(temp_start, temp_date, alpha=0.2, color="orange")
                temp_start = xc
                temp_label = value
            temp_date = xc

        if temp_label == True:
            ax.axvspan(temp_start, xc, alpha=0.2, color="orange")
        ## 카메라 찍기
        camera.snap()

    animation = camera.animate(500, blit=True)
    # .gif 파일로 저장하면 끝!
    animation.save(save_path, dpi=100, savefig_kwargs={"pad_inches": "tight"})


plot_sensor(df_test, df_test_label)


from sklearn.preprocessing import StandardScaler

# Skip data normalization because SMAP datas were already normalized.
df_train = df_train.fillna(method="ffill")
scaler = StandardScaler()
scaler.fit(df_train)
df_training_value = pd.DataFrame(scaler.transform(df_train))
print("Number of training samples:", len(df_training_value))


TIME_STEPS = 100

# Generated training sequences for use in the model.
def create_sequences(values, time_steps=TIME_STEPS):
    output = []
    for i in range(len(values) - time_steps + 1):
        output.append(values[i : (i + time_steps)])
    return np.stack(output)


x_train = create_sequences(df_training_value.values)
# x_valid = create_sequences(df_validating_value.values)
print("Training input shape: ", x_train.shape)
# print("Validating input shape: ", x_valid.shape)
NUM_OF_FEATURES = x_train.shape[-1]


model = keras.Sequential(
    [
        layers.InputLayer((x_train.shape[1], x_train.shape[2])),
        layers.Conv1D(filters=32, kernel_size=7, padding="same", strides=2, activation="relu"),
        layers.Dropout(rate=0.2),
        layers.Conv1D(filters=16, kernel_size=7, padding="same", strides=2, activation="relu"),
        layers.Conv1DTranspose(
            filters=16, kernel_size=7, padding="same", strides=2, activation="relu"
        ),
        layers.Dropout(rate=0.2),
        layers.Conv1DTranspose(
            filters=32, kernel_size=7, padding="same", strides=2, activation="relu"
        ),
        layers.Conv1DTranspose(filters=x_train.shape[2], kernel_size=7, padding="same"),
    ]
)
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss="mse")
model.summary()


history = model.fit(
    x_train,
    x_train,
    epochs=50,
    batch_size=128,
    validation_split=0.2,
    callbacks=[keras.callbacks.EarlyStopping(monitor="val_loss", patience=5, mode="min")],
)


plt.plot(history.history["loss"], label="Training Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.legend()
plt.show()


x_train_pred = model.predict(x_train)


# print(f'abs mean threshold: {compute_threshold(x_train, x_train_pred, option="abs_mean")}')
# print(f'abs median threshold: {compute_threshold(x_train, x_train_pred, option="abs_median")}')
# print(f'square mean threshold: {compute_threshold(x_train, x_train_pred, option="square_mean")}')
# print(f'square median threshold: {compute_threshold(x_train, x_train_pred, option="square_median")}')

# Choose the lowest loss of the anomaly datas((# of anomalies)th) as threshold by using rank option
threshold = compute_threshold(x_train, x_train_pred, option="abs_max")
print(f'rank threshold: {threshold}')


df_test_value = pd.DataFrame(scaler.transform(df_test))

# Create sequences from test values.
x_test = create_sequences(df_test_value.values)
print("Test input shape: ", x_test.shape)

# Get test MAE loss.
x_test_pred = model.predict(x_test)
test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)

# Detect all the samples which are anomalies.
# anomalies = test_mae_loss > threshold
anomalies = np.sum(test_mae_loss, axis=1) > threshold


fig = plt.figure(figsize=(16, 6))
ax = fig.add_subplot(111)

labels = df_test_label['label'].values.tolist()
dates = df_test.index


pd.DataFrame(np.sum(test_mae_loss, axis=1)).plot(ax=ax)
pd.DataFrame([threshold]*len(df_test)).plot(ax=ax)
ax.legend(["reconstruction error", "threshold"], loc="upper right")


temp_start = dates[0]
temp_date = dates[0]
temp_label = labels[0]

for xc, value in zip(dates, labels):
    if temp_label != value:
        if temp_label==True:
            ax.axvspan(temp_start, temp_date, alpha=0.2, color="orange")
        temp_start = xc
        temp_label = value
    temp_date = xc
if temp_label==True:
    ax.axvspan(temp_start, xc, alpha=0.2, color="orange")



# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies
anomalies = np.sum(test_mae_loss, axis=1) > threshold
print("Number of anomaly samples: ", np.sum(anomalies))

anomalous_data_indices = []
for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):
    if np.any(anomalies[data_idx - TIME_STEPS + 1 : data_idx]):
        anomalous_data_indices.append(data_idx)
print("Number of anomalous samples: ", len(anomalous_data_indices))


from sklearn.metrics import classification_report

test_pred = np.zeros(len(df_test))
test_label = df_test_label['label']
test_pred[anomalous_data_indices] = 1

accuracy = (np.sum(test_pred == test_label)) / len(test_label)
precision = (np.sum(test_pred * test_label)) / np.sum(test_pred)
recall = (np.sum(test_pred * test_label)) / np.sum(test_label)
f1 = 2 * (precision * recall) / (precision + recall + 1e-7)

print(f"accuracy: {accuracy*100:.2f}%")
print(f"precision: {precision*100:.2f}%")
print(f"recall: {recall*100:.2f}%")
print(f"f1: {f1*100:.2f}%")


from evaluator import evaluate

scores = evaluate(x_test, x_test_pred, df_test_label['label'], n=10, scoring='abs_mean')


print("f1:",np.max(scores['f1']))
