import json
import os
import random

import numpy as np
import pandas as pd
import sklearn
import tensorflow as tf
import tensorflow_probability as tfp
from celluloid import Camera
from matplotlib import pyplot as plt
from tensorflow import data, keras
from tensorflow.keras import backend as K
from tensorflow.keras import layers
from tqdm.notebook import tqdm, trange

# from sklearn.metrics import confusion_matrix

os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
os.environ["TF_FORCE_GPU_ALLOW_GROWTH"] = "true"


train_dir = "./datasets/SMAP/SMAP_train.npy"
test_dir = "./datasets/SMAP/SMAP_test.npy"
test_label_dir = "./datasets/SMAP/SMAP_test_label.npy"

np_train = np.load(train_dir)
np_test = np.load(test_dir)
np_test_label = np.load(test_label_dir)


df_train = pd.DataFrame(data=np_train, columns=[f"feature_{i+1}" for i in range(np_train.shape[1])])
df_test = pd.DataFrame(data=np_test, columns=[f"feature_{i+1}" for i in range(np_test.shape[1])])
df_test_label = pd.DataFrame(data=np_test_label, columns=["label"])


print(df_train.head())


fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)
df_train.plot(legend=False, ax=ax, subplots=True)
plt.show()


fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)
df_test.plot(legend=False, ax=ax, subplots=True)
plt.show()


from sklearn.preprocessing import StandardScaler

# Skip data normalization because SMAP datas were already normalized.
df_train = df_train.fillna(method="ffill")
scaler = StandardScaler()
scaler.fit(df_train)
df_training_value = pd.DataFrame(scaler.transform(df_train))
print("Number of training samples:", len(df_training_value))


TIME_STEPS = 1

# Generated training sequences for use in the model.
def create_sequences(values, time_steps=TIME_STEPS):
    output = []
    for i in range(len(values) - time_steps + 1):
        output.append(values[i : (i + time_steps)])
    return np.stack(output)


x_train = create_sequences(df_training_value.values)
print("Training input shape: ", x_train.shape)
NUM_OF_FEATURES = x_train.shape[-1]


class Sampling(layers.Layer):
    def __init__(self, name="sampling"):
        super(Sampling, self).__init__(name=name)

    def call(self, inputs):
        mu, logvar = inputs
        _, dim = mu.shape
        batch = 128
        sigma = K.exp(logvar * 0.5)
        epsilon = K.random_normal(shape=(batch, dim), mean=0.0, stddev=1.0)

        return mu + sigma * epsilon


class Encoder(layers.Layer):
    def __init__(self, time_step, x_dim, h_dim, z_dim, name="encoder", **kwargs):
        super(Encoder, self).__init__(name=name, **kwargs)
        self.encoder_inputs = keras.Input(shape=(time_step, x_dim))
        self.encoder_lstm = layers.LSTM(h_dim, activation="softplus", name="encoder_lstm")
        self.z_mean = layers.Dense(z_dim, name="z_mean")
        self.z_var = layers.Dense(z_dim, name="z_var")
        self.z_sample = Sampling()

    def call(self, inputs):
        print(inputs.shape)
        self.encoder_inputs = inputs  # (batch, time_steps, x_dim)
        hidden = self.encoder_lstm(self.encoder_inputs)  # (batch, h_dim)
        mu_z = self.z_mean(hidden)  # (batch, z_dim)
        logvar_z = self.z_var(hidden)  # (batch, z_dim)
        z = self.z_sample((mu_z, logvar_z))  # (batch, z_dim)

        return mu_z, logvar_z, z


class Decoder(layers.Layer):
    def __init__(self, time_step, x_dim, h_dim, z_dim, name="decoder", **kwargs):
        super(Decoder, self).__init__(name=name, **kwargs)

        self.decoder_inputs = layers.RepeatVector(time_step, name="repeat_vector")
        self.decoder_lstm = layers.LSTM(
            h_dim, activation="softplus", name="decoder_lstm", return_sequences=True
        )
        self.x_mean = layers.Dense(x_dim, name="x_mean")
        self.x_sigma = layers.Dense(x_dim, name="x_sigma", activation="tanh")

    def call(self, inputs):
        z = self.decoder_inputs(inputs)  # (batch, time_step, z_dim)
        hidden = self.decoder_lstm(z)  # (batch, time_step, h_dim)
        mu_x = self.x_mean(hidden)  # (batch, time_step, x_dim)
        sigma_x = self.x_sigma(hidden)  # (batch, time_step, x_dim)

        return mu_x, sigma_x


loss_metric = keras.metrics.Mean(name="loss")
likelihood_metric = keras.metrics.Mean(name="log likelihood")


class LSTM_VAE(keras.Model):
    def __init__(self, time_step, x_dim, h_dim, z_dim, name="lstm_vae", **kwargs):
        super(LSTM_VAE, self).__init__(name=name, **kwargs)

        self.encoder = Encoder(time_step, x_dim, h_dim, z_dim, **kwargs)
        self.decoder = Decoder(time_step, x_dim, h_dim, z_dim, **kwargs)

    def call(self, inputs):
        mu_z, logvar_z, z = self.encoder(inputs)  # (batch, z_dim)
        mu_x, sigma_x = self.decoder(z)  # (batch, time_step, x_dim)

        var_z = K.exp(logvar_z)
        kl_loss = K.mean(-0.5 * K.sum(var_z - logvar_z + tf.square(1 - mu_z), axis=1), axis=0)
        self.add_loss(kl_loss)
        dist = tfp.distributions.Normal(loc=mu_x, scale=tf.abs(sigma_x))
        log_px = -dist.log_prob(inputs)  # (batch, time_step, x_dim)
        return mu_x, sigma_x, log_px

    def reconstruct_loss(self, x, mu_x, sigma_x):
        var_x = K.square(sigma_x)
        reconst_loss = -0.5 * K.sum(K.log(var_x), axis=2) + K.sum(
            K.square(x - mu_x) / var_x, axis=2
        )
        reconst_loss = K.reshape(reconst_loss, shape=(-1, 1))
        return K.mean(reconst_loss, axis=0)

    def mean_log_likelihood(self, log_px):
        log_px = K.reshape(log_px, shape=(log_px.shape[0], log_px.shape[2]))
        mean_log_px = K.mean(log_px, axis=1)
        return K.mean(mean_log_px, axis=0)

    def train_step(self, data):
        if isinstance(data, tuple):
            x = data[0]
        else:
            x = data
        with tf.GradientTape() as tape:
            mu_x, sigma_x, log_px = self(x, training=True)
            loss = self.reconstruct_loss(x, mu_x, sigma_x)
            loss += sum(self.losses)
            mean_log_px = self.mean_log_likelihood(log_px)
        grads = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))
        loss_metric.update_state(loss)
        likelihood_metric.update_state(mean_log_px)

        return {"loss": loss_metric.result(), "log_likelihood": likelihood_metric.result()}


time_step = TIME_STEPS
x_dim = x_train.shape[2]
h_dim = 32
z_dim = 16

X_train = x_train.astype('float32')
model = LSTM_VAE(time_step, x_dim, h_dim, z_dim, dtype="float32")
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001))
train_dataset = data.Dataset.from_tensor_slices(X_train)
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(128, drop_remainder=True)

history = model.fit(train_dataset, epochs=50, shuffle=False).history
model.summary()


history = model.fit(
    x_train,
    x_train,
    epochs=50,
    batch_size=128,
    validation_split=0.2,
    callbacks=[keras.callbacks.EarlyStopping(monitor="val_loss", patience=5, mode="min")],
)


plt.plot(history.history["loss"], label="Training Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.legend()
plt.show()


# Get train MAE loss.
x_train_pred = model.predict(x_train)
train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)

# plt.hist(train_mae_loss, bins=50)
# plt.xlabel("Train MAE loss")
# plt.ylabel("No of samples")
# plt.show()

# Get reconstruction loss threshold.
threshold = np.max(train_mae_loss, axis=0)
print("Reconstruction error threshold: ", threshold)


# Checking how the first sequence is learnt
for i in range(1, 6):
    ax = plt.subplot(5, 1, i)
    plt.plot(x_train[0, :, i - 1])
    plt.plot(x_train_pred[0, :, i - 1])

plt.show()


df_test_value = pd.DataFrame(scaler.transform(df_test))
fig, ax = plt.subplots()
df_test_value.plot(legend=False, ax=ax)
plt.show()

# Create sequences from test values.
x_test = create_sequences(df_test_value.values)
print("Test input shape: ", x_test.shape)

# Get test MAE loss.
x_test_pred = model.predict(x_test)
test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)
# test_mae_loss = test_mae_loss.reshape((-1))

plt.hist(test_mae_loss, bins=50)
plt.xlabel("test MAE loss")
plt.ylabel("No of samples")
plt.show()


fig = plt.figure(figsize=(16, 6))
ax = fig.add_subplot(111)

labels = df_test_label.values.tolist()
dates = df_test.index


np.sum(test_mae_loss, axis=1).plot(ax=ax)
ax.legend(["reconstruction error"], loc="upper right")

## 고장구간 표시
temp_start = dates[0]
temp_date = dates[0]
temp_label = labels[0]

for xc, value in zip(dates, labels):
    if temp_label != value:
        if not temp_label:
            ax.axvspan(temp_start, temp_date, alpha=0.2, color="orange")
        temp_start = xc
        temp_label = value
    temp_date = xc
if not temp_label:
    ax.axvspan(temp_start, xc, alpha=0.2, color="orange")


# Detect all the samples which are anomalies.
# anomalies = test_mae_loss > threshold
print(np.sum(test_mae_loss, axis=1).shape, threshold.shape)
anomalies = np.sum(test_mae_loss > threshold
print("Number of anomaly samples: ", np.sum(anomalies))
print("Indices of anomaly samples: ", np.where(anomalies))


# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies
anomalous_data_indices = []
for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):
    if np.all(anomalies[data_idx - TIME_STEPS + 1 : data_idx]):
        anomalous_data_indices.append(data_idx)


df_subset = df_test["feature_1"].iloc[anomalous_data_indices]
fig, ax = plt.subplots(figsize=(16, 6))
df_test["feature_1"].plot(legend=False, ax=ax)
df_subset.plot(legend=False, ax=ax, color="r")
plt.show()


from sklearn.metrics import classification_report

test_pred = np.zeros(len(df_test // 2))
test_label = df_test_label
test_pred[anomalous_data_indices] = 1

print(classification_report(test_label, test_pred))
# accuracy = (np.sum(test_pred == test_label)) / len(test_label) * 100
# precision = (np.sum(test_pred * test_label)) / np.sum(test_pred) * 100
# recall = (np.sum(test_pred * test_label)) / np.sum(test_label) * 100

# print(f"accuracy: {accuracy:.2f}%")
# print(f"precision: {precision:.2f}%")
# print(f"recall: {recall:.2f}%")
