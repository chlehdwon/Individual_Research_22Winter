{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQDOPjL61Y0K"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This script demonstrates how you can use a RNN-based model to detect anomalies in timeseries data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZQ1Zy1h1Y0L"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T17:55:20.664453Z",
     "iopub.status.busy": "2023-02-11T17:55:20.663780Z",
     "iopub.status.idle": "2023-02-11T17:55:22.425255Z",
     "shell.execute_reply": "2023-02-11T17:55:22.424479Z",
     "shell.execute_reply.started": "2023-02-11T17:55:20.664406Z"
    },
    "id": "0FfTR9ku1Y0L",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-12 02:55:21.573810: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from celluloid import Camera\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from evaluator import evaluate, compute_threshold\n",
    "from dataloader import loader\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9sE3S3G1Y0M"
   },
   "source": [
    "# Load the data\n",
    "\n",
    "We will use the SMAP data sets for training and testing.\n",
    "\n",
    "You can find raw data in https://s3-us-west-2.amazonaws.com/telemanom/data.ziphttps://s3-us-west-2.amazonaws.com/telemanom/data.zip\n",
    "\n",
    "I used preprocessed datasets with 25 dimensions.\n",
    "\n",
    "You can find preprocessed data in https://drive.google.com/drive/folders/1gisthCoE-RrKJ0j3KPV7xiibhHWT9qRmhttps://drive.google.com/drive/folders/1gisthCoE-RrKJ0j3KPV7xiibhHWT9qRm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T17:55:22.426516Z",
     "iopub.status.busy": "2023-02-11T17:55:22.426092Z",
     "iopub.status.idle": "2023-02-11T17:55:22.452828Z",
     "shell.execute_reply": "2023-02-11T17:55:22.451897Z",
     "shell.execute_reply.started": "2023-02-11T17:55:22.426495Z"
    },
    "id": "EtSnZuX_1Y0M",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train, df_test, df_test_label = loader(dataset=\"UCR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PZGCGNM1Y0O"
   },
   "source": [
    "## Prepare training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T17:55:22.454487Z",
     "iopub.status.busy": "2023-02-11T17:55:22.454198Z",
     "iopub.status.idle": "2023-02-11T17:55:22.461331Z",
     "shell.execute_reply": "2023-02-11T17:55:22.460664Z",
     "shell.execute_reply.started": "2023-02-11T17:55:22.454471Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 35000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Skip data normalization because SMAP datas were already normalized.\n",
    "df_train = df_train.fillna(method=\"ffill\")\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train)\n",
    "df_training_value = pd.DataFrame(scaler.transform(df_train))\n",
    "print(\"Number of training samples:\", len(df_training_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76o15gMK1Y0P"
   },
   "source": [
    "### Create sequences\n",
    "Create sequences combining `TIME_STEPS` contiguous data values from the\n",
    "training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T17:55:22.462273Z",
     "iopub.status.busy": "2023-02-11T17:55:22.462008Z",
     "iopub.status.idle": "2023-02-11T17:55:22.508152Z",
     "shell.execute_reply": "2023-02-11T17:55:22.507432Z",
     "shell.execute_reply.started": "2023-02-11T17:55:22.462257Z"
    },
    "id": "Z_wb3M4Q1Y0P",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input shape:  (34951, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "TIME_STEPS = 50\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps=TIME_STEPS):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "\n",
    "x_train = create_sequences(df_training_value.values)\n",
    "# x_valid = create_sequences(df_validating_value.values)\n",
    "print(\"Training input shape: \", x_train.shape)\n",
    "# print(\"Validating input shape: \", x_valid.shape)\n",
    "NUM_OF_FEATURES = x_train.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnBIFj7w1Y0P"
   },
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T17:55:22.509162Z",
     "iopub.status.busy": "2023-02-11T17:55:22.508881Z",
     "iopub.status.idle": "2023-02-11T17:55:23.650486Z",
     "shell.execute_reply": "2023-02-11T17:55:23.649741Z",
     "shell.execute_reply.started": "2023-02-11T17:55:22.509146Z"
    },
    "id": "aQ4kjOK31Y0P",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-12 02:55:22.520702: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-02-12 02:55:22.567108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:1f:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6\n",
      "coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s\n",
      "2023-02-12 02:55:22.567154: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-02-12 02:55:22.569805: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-02-12 02:55:22.569877: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-02-12 02:55:22.570683: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2023-02-12 02:55:22.570936: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2023-02-12 02:55:22.571655: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-02-12 02:55:22.572254: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-02-12 02:55:22.572409: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-02-12 02:55:22.573016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-02-12 02:55:22.573324: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-12 02:55:22.576491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:1f:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6\n",
      "coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s\n",
      "2023-02-12 02:55:22.577034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-02-12 02:55:22.577081: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-02-12 02:55:23.069264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-02-12 02:55:23.069298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2023-02-12 02:55:23.069304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2023-02-12 02:55:23.070162: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-02-12 02:55:23.070202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 18442 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:1f:00.0, compute capability: 8.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50, 64)            16896     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 32)            8320      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                3250      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 50, 1)             0         \n",
      "=================================================================\n",
      "Total params: 65,714\n",
      "Trainable params: 65,714\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.InputLayer(input_shape=(x_train.shape[1], x_train.shape[2])),\n",
    "        layers.LSTM(64, return_sequences=True),\n",
    "        layers.LSTM(32),\n",
    "        layers.RepeatVector(x_train.shape[1]),\n",
    "        layers.LSTM(32, return_sequences=True),\n",
    "        layers.LSTM(64),\n",
    "        layers.Dense(x_train.shape[1] * x_train.shape[2]),\n",
    "        layers.Reshape([x_train.shape[1], x_train.shape[2]]),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Unm8M4T71Y0P"
   },
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T17:55:23.652652Z",
     "iopub.status.busy": "2023-02-11T17:55:23.652348Z"
    },
    "id": "Edtgm3i81Y0Q",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-12 02:55:23.679542: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-02-12 02:55:23.699846: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2900000000 Hz\n",
      "2023-02-12 02:55:26.656842: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-02-12 02:55:27.314780: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8201\n",
      "2023-02-12 02:55:27.582409: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-02-12 02:55:28.220434: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-02-12 02:55:28.220510: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 10s 23ms/step - loss: 0.2309 - val_loss: 0.1336\n",
      "Epoch 2/50\n",
      "219/219 [==============================] - 4s 18ms/step - loss: 0.0456 - val_loss: 0.0971\n",
      "Epoch 3/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0349 - val_loss: 0.0884\n",
      "Epoch 4/50\n",
      "219/219 [==============================] - 4s 18ms/step - loss: 0.0299 - val_loss: 0.0770\n",
      "Epoch 5/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0262 - val_loss: 0.0710\n",
      "Epoch 6/50\n",
      "219/219 [==============================] - 4s 18ms/step - loss: 0.0256 - val_loss: 0.0689\n",
      "Epoch 7/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0225 - val_loss: 0.0659\n",
      "Epoch 8/50\n",
      "219/219 [==============================] - 4s 18ms/step - loss: 0.0207 - val_loss: 0.0623\n",
      "Epoch 9/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0194 - val_loss: 0.0654\n",
      "Epoch 10/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0188 - val_loss: 0.0623\n",
      "Epoch 11/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0180 - val_loss: 0.0560\n",
      "Epoch 12/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0179 - val_loss: 0.0537\n",
      "Epoch 13/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0168 - val_loss: 0.0478\n",
      "Epoch 14/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0164 - val_loss: 0.0459\n",
      "Epoch 15/50\n",
      "219/219 [==============================] - 4s 18ms/step - loss: 0.0157 - val_loss: 0.0452\n",
      "Epoch 16/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0157 - val_loss: 0.0469\n",
      "Epoch 17/50\n",
      "219/219 [==============================] - 4s 18ms/step - loss: 0.0159 - val_loss: 0.0466\n",
      "Epoch 18/50\n",
      "219/219 [==============================] - 4s 20ms/step - loss: 0.0132 - val_loss: 0.0412\n",
      "Epoch 19/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0146 - val_loss: 0.0392\n",
      "Epoch 20/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0126 - val_loss: 0.0392\n",
      "Epoch 21/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0134 - val_loss: 0.0373\n",
      "Epoch 22/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0126 - val_loss: 0.0333\n",
      "Epoch 23/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0115 - val_loss: 0.0408\n",
      "Epoch 24/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0124 - val_loss: 0.0303\n",
      "Epoch 25/50\n",
      "219/219 [==============================] - 4s 18ms/step - loss: 0.0105 - val_loss: 0.0344\n",
      "Epoch 26/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0113 - val_loss: 0.0288\n",
      "Epoch 27/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0108 - val_loss: 0.0329\n",
      "Epoch 28/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0109 - val_loss: 0.0325\n",
      "Epoch 29/50\n",
      "219/219 [==============================] - 4s 18ms/step - loss: 0.0087 - val_loss: 0.0293\n",
      "Epoch 30/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0098 - val_loss: 0.0298\n",
      "Epoch 31/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0089 - val_loss: 0.0246\n",
      "Epoch 32/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0101 - val_loss: 0.0257\n",
      "Epoch 33/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0087 - val_loss: 0.0233\n",
      "Epoch 34/50\n",
      "219/219 [==============================] - 4s 18ms/step - loss: 0.0087 - val_loss: 0.0225\n",
      "Epoch 35/50\n",
      "219/219 [==============================] - 4s 20ms/step - loss: 0.0072 - val_loss: 0.0229\n",
      "Epoch 36/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0078 - val_loss: 0.0218\n",
      "Epoch 37/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0079 - val_loss: 0.0219\n",
      "Epoch 38/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0073 - val_loss: 0.0248\n",
      "Epoch 39/50\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.0076 - val_loss: 0.0231\n",
      "Epoch 40/50\n",
      " 84/219 [==========>...................] - ETA: 2s - loss: 0.0076"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjbNUGBv1Y0Q"
   },
   "source": [
    "Let's plot training and validation loss to see how the training went."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpV5vGMj1Y0Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUyjNuQk1Y0Q",
    "tags": []
   },
   "source": [
    "## Detecting anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train_pred = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f'abs mean threshold: {compute_threshold(x_train, x_train_pred, option=\"abs_mean\")}')\n",
    "# print(f'abs median threshold: {compute_threshold(x_train, x_train_pred, option=\"abs_median\")}')\n",
    "# print(f'square mean threshold: {compute_threshold(x_train, x_train_pred, option=\"square_mean\")}')\n",
    "# print(f'square median threshold: {compute_threshold(x_train, x_train_pred, option=\"square_median\")}')\n",
    "\n",
    "# Choose the lowest loss of the anomaly datas((# of anomalies)th) as threshold by using rank option\n",
    "threshold = compute_threshold(x_train, x_train_pred, option=\"abs_max\")\n",
    "print(f'rank threshold: {threshold}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-YQD_hO1Y0R",
    "tags": []
   },
   "source": [
    "### Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test_value = pd.DataFrame(scaler.transform(df_test))\n",
    "\n",
    "# Create sequences from test values.\n",
    "x_test = create_sequences(df_test_value.values)\n",
    "print(\"Test input shape: \", x_test.shape)\n",
    "\n",
    "# Get test MAE loss.\n",
    "x_test_pred = model.predict(x_test)\n",
    "test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\n",
    "\n",
    "# Detect all the samples which are anomalies.\n",
    "# anomalies = test_mae_loss > threshold\n",
    "anomalies = np.sum(test_mae_loss, axis=1) > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "labels = df_test_label['label'].values.tolist()\n",
    "dates = df_test.index\n",
    "\n",
    "\n",
    "pd.DataFrame(np.sum(test_mae_loss, axis=1)).plot(ax=ax)\n",
    "pd.DataFrame([threshold]*len(df_test)).plot(ax=ax)\n",
    "ax.legend([\"reconstruction error\", \"threshold\"], loc=\"upper right\")\n",
    "\n",
    "\n",
    "temp_start = dates[0]\n",
    "temp_date = dates[0]\n",
    "temp_label = labels[0]\n",
    "\n",
    "for xc, value in zip(dates, labels):\n",
    "    if temp_label != value:\n",
    "        if temp_label==True:\n",
    "            ax.axvspan(temp_start, temp_date, alpha=0.2, color=\"orange\")\n",
    "        temp_start = xc\n",
    "        temp_label = value\n",
    "    temp_date = xc\n",
    "if temp_label==True:\n",
    "    ax.axvspan(temp_start, xc, alpha=0.2, color=\"orange\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T12:38:01.255924Z",
     "iopub.status.busy": "2023-02-11T12:38:01.255364Z",
     "iopub.status.idle": "2023-02-11T12:38:01.261595Z",
     "shell.execute_reply": "2023-02-11T12:38:01.260128Z",
     "shell.execute_reply.started": "2023-02-11T12:38:01.255879Z"
    },
    "tags": []
   },
   "source": [
    "## #1 Set threshold by using evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKbjg85P1Y0R",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies\n",
    "anomalies = np.sum(test_mae_loss, axis=1) > threshold\n",
    "print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
    "\n",
    "anomalous_data_indices = []\n",
    "for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):\n",
    "    if np.any(anomalies[data_idx - TIME_STEPS + 1 : data_idx]):\n",
    "        anomalous_data_indices.append(data_idx)\n",
    "print(\"Number of anomalous samples: \", len(anomalous_data_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_pred = np.zeros(len(df_test))\n",
    "test_label = df_test_label['label']\n",
    "test_pred[anomalous_data_indices] = 1\n",
    "\n",
    "accuracy = (np.sum(test_pred == test_label)) / len(test_label)\n",
    "precision = (np.sum(test_pred * test_label)) / np.sum(test_pred)\n",
    "recall = (np.sum(test_pred * test_label)) / np.sum(test_label)\n",
    "f1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "\n",
    "print(f\"accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"precision: {precision*100:.2f}%\")\n",
    "print(f\"recall: {recall*100:.2f}%\")\n",
    "print(f\"f1: {f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kW_E_0AZ1Y0R"
   },
   "source": [
    "### #2 we could find the threshold with highest f1 score by calculating all cases \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from evaluator import evaluate\n",
    "\n",
    "scores = evaluate(x_test, x_test_pred, df_test_label['label'], n=10, scoring='abs_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"f1:\",np.max(scores['f1']))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "timeseries_anomaly_detection",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "cs495_winter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
